{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7aa3a470-802f-4b0a-a0df-b01b7aed0ca7",
   "metadata": {},
   "source": [
    "# RTF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "26c243c8-97a7-4a4f-8d03-4120c684d9ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import psutil\n",
    "import zipfile\n",
    "import chardet\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import balanced_accuracy_score\n",
    "# from striprtf.striprtf import rtf_to_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "125c466a-38df-499a-ba8c-41f5d95be7ee",
   "metadata": {},
   "source": [
    "## 0 Current RAM usage monitoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "86c01b53-9321-456a-980b-93797d029984",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current RAM usage: 1.61 / 24 GB (6.7 %)\n"
     ]
    }
   ],
   "source": [
    "def get_ram_usage():\n",
    "    memory_info = psutil.Process().memory_info()\n",
    "    return memory_info.rss / (1024 * 1024 * 1024)  # Resident Set Size (RSS) in bytes\n",
    "\n",
    "print(f\"Current RAM usage: {get_ram_usage():.2f} / 24 GB ({get_ram_usage()/24*100:.1f} %)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3655922-9505-431e-ab07-a10a3fa10c04",
   "metadata": {},
   "source": [
    "## 1 Preprocess data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cdc11823-e81b-48b4-b6a7-8a0f5e7f547d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "##### Get data -> Try decoding it with the second read_rtf_file commented function #####\n",
    "\n",
    "# List of the .rtf filenames of our train or test dataset\n",
    "rtf_filenames_train = zipfile.ZipFile('rtf-train.zip').namelist()[:-1]\n",
    "decode = True\n",
    "n = 2 # Value for n-grams\n",
    "\n",
    "# Given a path, returns an .rtf as string\n",
    "def read_rtf_file(file_path):\n",
    "    # Read file in bianary\n",
    "    with open(file_path, 'r') as file:\n",
    "        rtf_content = file.read()\n",
    "    # text = rtf_to_text(rtf_content)\n",
    "    return rtf_content\n",
    "\n",
    "# Given a path, returns an .rtf as string after having decoded it\n",
    "def read_rtf_file_decoded(file_path):\n",
    "    # Read file in bianary\n",
    "    with open(file_path, 'rb') as file:\n",
    "        rtf_content = file.read()\n",
    "    # Decode the file\n",
    "    encoding = chardet.detect(rtf_content)[\"encoding\"]\n",
    "    rtf_content = rtf_content.decode(encoding if encoding != None else 'utf-8')\n",
    "    return rtf_content\n",
    "\n",
    "def get_rtf_generator(rtf_filenames_train, labels_train, decoded_indices_train, decode, train=True):\n",
    "    errors_train = 0\n",
    "    for i, rtfname in enumerate(np.array(rtf_filenames_train)[decoded_indices_train.astype(bool)]): # Only the decoded filenames\n",
    "        if not train and i in [147, 150]:\n",
    "            print(i, rtfname, 'Refuse to decode: too long')\n",
    "            errors_train += 1\n",
    "            decoded_indices_train[i] = 0\n",
    "            continue\n",
    "        try:\n",
    "            rtf = read_rtf_file_decoded(rtfname) if decode else read_rtf_file(rtfname)\n",
    "            labels_train += [int(rtfname[-1])] if train else []   \n",
    "            # if i % 100 == 0:\n",
    "            #     print(f'get_rtf_generator() iter {i} done')\n",
    "            yield rtf\n",
    "        except UnicodeDecodeError:\n",
    "            # Deal with decoding error\n",
    "            print(i, rtfname, 'UnicodeDecodeError')\n",
    "            errors_train += 1\n",
    "            decoded_indices_train[i] = 0\n",
    "            continue\n",
    "        except TypeError:\n",
    "            print(i, rtfname, 'TypeError')\n",
    "            errors_train += 1\n",
    "            decoded_indices_train[i] = 0\n",
    "            continue\n",
    "        except KeyError:\n",
    "            print(i, rtfname, 'KeyError')\n",
    "            errors_train += 1\n",
    "            decoded_indices_train[i] = 0\n",
    "            continue\n",
    "    print('Number of non-decoded sequences:', errors_train)\n",
    "    print('Percentage of non-decoded sequences:', round(errors_train/len(rtf_filenames_train)*100, 2), '%')\n",
    "\n",
    "\n",
    "# # labels_train, decoded_indices_train = get_decoded_indices_labels(rtf_filenames_train, decode=decode)\n",
    "# labels_train, decoded_indices_train = [], np.ones(len(rtf_filenames_train))\n",
    "# generator_rtf_train = get_rtf_generator(rtf_filenames_train, labels_train, decoded_indices_train, decode=decode, train=True)\n",
    "\n",
    "\n",
    "# ##### Extract features #####\n",
    "\n",
    "# vectorizer = CountVectorizer(strip_accents='unicode', lowercase=True, ngram_range=(n,n), analyzer='char', min_df=1)\n",
    "# features_train = vectorizer.fit_transform(generator_rtf_train)\n",
    "# features_train = features_train.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8e79fe41-774e-40b6-972a-462496136b24",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "features_train.shape = (3809, 47298)\n",
      "len(labels_train) = 3809\n"
     ]
    }
   ],
   "source": [
    "##### Save/load data #####\n",
    "\n",
    "# np.save('features_train_n2_df1.npy', features_train)\n",
    "# np.save('labels_train.npy', labels_train)\n",
    "features_train = np.load('features_train_n2_df1.npy')\n",
    "labels_train = np.load('labels_train.npy')\n",
    "print(f'{features_train.shape = }')\n",
    "print(f'{len(labels_train) = }')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ada0c9b-5dd9-4005-ace1-5286e032b1ff",
   "metadata": {},
   "source": [
    "## 2 Train model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25dcd8de-5983-4544-b893-e1dfef2eed08",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### 2.0 K-fold cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d56c45c3-6d27-415d-99ac-2b099dd90120",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_evaluate(model, features, labels, model_name='base-model', n_splits=5, fit_whole_dataset=True):\n",
    "    # K-Fold CV\n",
    "    kf = KFold(n_splits=n_splits, shuffle=True, random_state=0)\n",
    "    accuracy_list, var_list = [], []\n",
    "    split = 0\n",
    "    \n",
    "    # Training + metrics\n",
    "    for idx_train, idx_eval in kf.split(features):\n",
    "        X_train, X_eval, y_train, y_eval = features[idx_train], features[idx_eval], labels[idx_train], labels[idx_eval]\n",
    "        # Fit the model\n",
    "        model.fit(X_train, y_train)\n",
    "        # Predict\n",
    "        y_pred = model.predict(X_eval)\n",
    "        # Evaluate \n",
    "        acc = balanced_accuracy_score(y_eval, y_pred)\n",
    "        accuracy_list += [acc*len(y_eval)]\n",
    "        var_list += [acc]\n",
    "        print(f'Split {split} done')\n",
    "        split += 1\n",
    "        \n",
    "    # Compute CV_score\n",
    "    cvscore = sum(accuracy_list)/len(labels)\n",
    "    variance = np.std(var_list)\n",
    "    print(f'{model_name}: CV-score = {cvscore:.3f}, Variance = {variance:.4f}\\n')\n",
    "\n",
    "    # Train the model on the whole Train dataset\n",
    "    if fit_whole_dataset:\n",
    "        model.fit(features, labels)\n",
    "    \n",
    "    return cvscore, variance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a258edc7-80fb-42e7-a6a3-7fa6660d8dbd",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### 2.1 `n-gram==1` (500 training ex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f3f7318f-beab-470e-afca-d1bbb8f0e6cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split 0 done\n",
      "Split 1 done\n",
      "Split 2 done\n",
      "Split 3 done\n",
      "Split 4 done\n",
      "GBC() n-gram=1: CV-score = 0.951, Variance = 0.0402\n",
      "\n",
      "CPU times: user 2.02 s, sys: 0 ns, total: 2.02 s\n",
      "Wall time: 2.01 s\n"
     ]
    }
   ],
   "source": [
    "# Gradient Boosting\n",
    "\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "gb_model = GradientBoostingClassifier()\n",
    "%time _ = train_evaluate(gb_model, features_train, np.array(labels_train), model_name=\"GBC() n-gram=1\", n_splits=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "68d8be1c-c0b3-471b-82b7-e5584523fd8b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split 0 done\n",
      "Split 1 done\n",
      "Split 2 done\n",
      "Split 3 done\n",
      "Split 4 done\n",
      "RFC() n-gram=1: CV-score = 0.927, Variance = 0.0388\n",
      "\n",
      "CPU times: user 720 ms, sys: 339 Âµs, total: 720 ms\n",
      "Wall time: 717 ms\n"
     ]
    }
   ],
   "source": [
    "# Random Forest\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rf_model = RandomForestClassifier()\n",
    "%time _ = train_evaluate(rf_model, features_train, np.array(labels_train), model_name=\"RFC() n-gram=1\", n_splits=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f5517a0b-fe83-43ea-891d-02d807a3d010",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split 0 done\n",
      "Split 1 done\n",
      "Split 2 done\n",
      "Split 3 done\n",
      "Split 4 done\n",
      "DTC() n-gram=1: CV-score = 0.917, Variance = 0.0431\n",
      "\n",
      "CPU times: user 45.3 ms, sys: 774 Âµs, total: 46 ms\n",
      "Wall time: 44.5 ms\n"
     ]
    }
   ],
   "source": [
    "# Decision Tree\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "dt_model = DecisionTreeClassifier()\n",
    "%time _ = train_evaluate(dt_model, features_train, np.array(labels_train), model_name=\"DTC() n-gram=1\", n_splits=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "0394b4f0-8306-4019-b5ad-c8357b143f60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split 0 done\n",
      "Split 1 done\n",
      "Split 2 done\n",
      "Split 3 done\n",
      "Split 4 done\n",
      "SVC() n-gram=1: CV-score = 0.759, Variance = 0.0389\n",
      "\n",
      "CPU times: user 22.8 ms, sys: 0 ns, total: 22.8 ms\n",
      "Wall time: 22 ms\n"
     ]
    }
   ],
   "source": [
    "# SVM\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "svc_model = SVC()\n",
    "%time _ = train_evaluate(svc_model, features_train, np.array(labels_train), model_name=\"SVC() n-gram=1\", n_splits=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8e43955-057b-43d7-a90f-a3c0ef576b38",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### 2.2 `n-gram==2` (500 training ex): BEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "f80b8ca7-71b6-4410-9cc9-c76d13a91b69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split 0 done\n",
      "Split 1 done\n",
      "Split 2 done\n",
      "Split 3 done\n",
      "Split 4 done\n",
      "GBC() n-gram=2: CV-score = 0.958, Variance = 0.0292\n",
      "\n",
      "CPU times: user 22 s, sys: 7.69 ms, total: 22 s\n",
      "Wall time: 22 s\n"
     ]
    }
   ],
   "source": [
    "# Gradient Boosting\n",
    "\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "gb_model = GradientBoostingClassifier()\n",
    "%time _ = train_evaluate(gb_model, features_train, np.array(labels_train), model_name=\"GBC() n-gram=2\", n_splits=5, fit_whole_dataset=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8e12c93f-0882-4535-9e25-460ec873b14b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split 0 done\n",
      "Split 1 done\n",
      "Split 2 done\n",
      "Split 3 done\n",
      "Split 4 done\n",
      "RFC() n-gram=2: CV-score = 0.921, Variance = 0.0423\n",
      "\n",
      "CPU times: user 1.05 s, sys: 3.18 ms, total: 1.05 s\n",
      "Wall time: 1.05 s\n"
     ]
    }
   ],
   "source": [
    "# Random Forest\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rf_model = RandomForestClassifier()\n",
    "%time _ = train_evaluate(rf_model, features_train, np.array(labels_train), model_name=\"RFC() n-gram=2\", n_splits=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ff37ec48-d1d2-48ed-ba55-bdeb338cb7fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split 0 done\n",
      "Split 1 done\n",
      "Split 2 done\n",
      "Split 3 done\n",
      "Split 4 done\n",
      "DTC() n-gram=2: CV-score = 0.938, Variance = 0.0416\n",
      "\n",
      "CPU times: user 296 ms, sys: 438 Âµs, total: 297 ms\n",
      "Wall time: 294 ms\n"
     ]
    }
   ],
   "source": [
    "# Decision Tree\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "dt_model = DecisionTreeClassifier()\n",
    "%time _ = train_evaluate(dt_model, features_train, np.array(labels_train), model_name=\"DTC() n-gram=2\", n_splits=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "80f9f903-7394-48fc-ac34-d80db9b4869d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split 0 done\n",
      "Split 1 done\n",
      "Split 2 done\n",
      "Split 3 done\n",
      "Split 4 done\n",
      "SVC() n-gram=2: CV-score = 0.777, Variance = 0.0486\n",
      "\n",
      "CPU times: user 125 ms, sys: 59 Âµs, total: 125 ms\n",
      "Wall time: 122 ms\n"
     ]
    }
   ],
   "source": [
    "# SVM\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "svc_model = SVC()\n",
    "%time _ = train_evaluate(svc_model, features_train, np.array(labels_train), model_name=\"SVC() n-gram=2\", n_splits=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aee9e9e-6ddf-4445-9929-9089eeddc8bc",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### 2.3 `n-gram==3` (500 training ex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "56fa5e77-49ea-4573-ad3d-eba95a4f9c92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split 0 done\n",
      "Split 1 done\n",
      "Split 2 done\n",
      "Split 3 done\n",
      "Split 4 done\n",
      "GBC() n-gram=3: CV-score = 0.952, Variance = 0.0374\n",
      "\n",
      "CPU times: user 1min 27s, sys: 18.5 ms, total: 1min 27s\n",
      "Wall time: 1min 27s\n"
     ]
    }
   ],
   "source": [
    "# Gradient Boosting\n",
    "\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "gb_model = GradientBoostingClassifier()\n",
    "%time _ = train_evaluate(gb_model, features_train, np.array(labels_train), model_name=\"GBC() n-gram=3\", n_splits=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "38fb9552-6b17-4ed2-aef3-ea259a5deccc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split 0 done\n",
      "Split 1 done\n",
      "Split 2 done\n",
      "Split 3 done\n",
      "Split 4 done\n",
      "RFC() n-gram=3: CV-score = 0.927, Variance = 0.0351\n",
      "\n",
      "CPU times: user 1.55 s, sys: 15.8 ms, total: 1.57 s\n",
      "Wall time: 1.56 s\n"
     ]
    }
   ],
   "source": [
    "# Random Forest\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rf_model = RandomForestClassifier()\n",
    "%time _ = train_evaluate(rf_model, features_train, np.array(labels_train), model_name=\"RFC() n-gram=3\", n_splits=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bcffaf98-4d97-459d-8818-f70e5f189592",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split 0 done\n",
      "Split 1 done\n",
      "Split 2 done\n",
      "Split 3 done\n",
      "Split 4 done\n",
      "DTC() n-gram=3: CV-score = 0.937, Variance = 0.0524\n",
      "\n",
      "CPU times: user 1.35 s, sys: 15.9 ms, total: 1.36 s\n",
      "Wall time: 1.36 s\n"
     ]
    }
   ],
   "source": [
    "# Decision Tree\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "dt_model = DecisionTreeClassifier()\n",
    "%time _ = train_evaluate(dt_model, features_train, np.array(labels_train), model_name=\"DTC() n-gram=3\", n_splits=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8c4005bc-baea-4bbc-b4e4-f30b73b19037",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split 0 done\n",
      "Split 1 done\n",
      "Split 2 done\n",
      "Split 3 done\n",
      "Split 4 done\n",
      "SVC() n-gram=3: CV-score = 0.713, Variance = 0.0639\n",
      "\n",
      "CPU times: user 9.56 s, sys: 16.9 s, total: 26.5 s\n",
      "Wall time: 2.67 s\n"
     ]
    }
   ],
   "source": [
    "# SVM\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "svc_model = SVC()\n",
    "%time _ = train_evaluate(svc_model, features_train, np.array(labels_train), model_name=\"SVC() n-gram=3\", n_splits=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bcb5df0-366b-4b6e-a1d8-72b9d34c9ab9",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### 2.4 `n-gram==4` (500 training ex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "1c5d77bb-2d47-43e6-a546-c9aaf08953b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split 0 done\n",
      "Split 1 done\n",
      "Split 2 done\n",
      "Split 3 done\n",
      "Split 4 done\n",
      "GBC() n-gram=4: CV-score = 0.943, Variance = 0.0453\n",
      "\n",
      "CPU times: user 9min 6s, sys: 221 ms, total: 9min 6s\n",
      "Wall time: 9min 6s\n"
     ]
    }
   ],
   "source": [
    "# Gradient Boosting\n",
    "\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "gb_model = GradientBoostingClassifier()\n",
    "%time _ = train_evaluate(gb_model, features_train, np.array(labels_train), model_name=\"GBC() n-gram=4\", n_splits=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "37d015c0-16fb-40e8-9b30-0c0654922932",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split 0 done\n",
      "Split 1 done\n",
      "Split 2 done\n",
      "Split 3 done\n",
      "Split 4 done\n",
      "RFC() n-gram=4: CV-score = 0.934, Variance = 0.0287\n",
      "\n",
      "CPU times: user 3.26 s, sys: 144 ms, total: 3.4 s\n",
      "Wall time: 3.4 s\n"
     ]
    }
   ],
   "source": [
    "# Random Forest\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rf_model = RandomForestClassifier()\n",
    "%time _ = train_evaluate(rf_model, features_train, np.array(labels_train), model_name=\"RFC() n-gram=4\", n_splits=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "7560e2f0-47a3-4970-a284-bc70d4cc05ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split 0 done\n",
      "Split 1 done\n",
      "Split 2 done\n",
      "Split 3 done\n",
      "Split 4 done\n",
      "DTC() n-gram=4: CV-score = 0.944, Variance = 0.0297\n",
      "\n",
      "CPU times: user 6.73 s, sys: 160 ms, total: 6.89 s\n",
      "Wall time: 6.89 s\n"
     ]
    }
   ],
   "source": [
    "# Decision Tree\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "dt_model = DecisionTreeClassifier()\n",
    "%time _ = train_evaluate(dt_model, features_train, np.array(labels_train), model_name=\"DTC() n-gram=4\", n_splits=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "b70cce2d-d5ee-449c-a9fa-fbd8a1921ff6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split 0 done\n",
      "Split 1 done\n",
      "Split 2 done\n",
      "Split 3 done\n",
      "Split 4 done\n",
      "SVC() n-gram=4: CV-score = 0.691, Variance = 0.0509\n",
      "\n",
      "CPU times: user 56.3 s, sys: 1min 16s, total: 2min 12s\n",
      "Wall time: 13.6 s\n"
     ]
    }
   ],
   "source": [
    "# SVM\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "svc_model = SVC()\n",
    "%time _ = train_evaluate(svc_model, features_train, np.array(labels_train), model_name=\"SVC() n-gram=4\", n_splits=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4eec3b8-2865-4eca-86c3-f9c573576ba9",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### 2.5 `analyzer='word'` and `n-gram==1` (500 training ex)Â¶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "470799f2-a3e4-4e16-9f3a-786b5572d2b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split 0 done\n",
      "Split 1 done\n",
      "Split 2 done\n",
      "Split 3 done\n",
      "Split 4 done\n",
      "GBC() n-gram=1: CV-score = 0.931, Variance = 0.0567\n",
      "\n",
      "CPU times: user 7.33 s, sys: 108 Âµs, total: 7.33 s\n",
      "Wall time: 7.32 s\n"
     ]
    }
   ],
   "source": [
    "# Gradient Boosting\n",
    "\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "gb_model = GradientBoostingClassifier()\n",
    "%time _ = train_evaluate(gb_model, features_train, np.array(labels_train), model_name=f\"GBC() n-gram={n}\", n_splits=5, fit_whole_dataset=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "a1350ca6-815c-4d6f-a529-93f394ddf8ff",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split 0 done\n",
      "Split 1 done\n",
      "Split 2 done\n",
      "Split 3 done\n",
      "Split 4 done\n",
      "RFC() n-gram=1: CV-score = 0.936, Variance = 0.0377\n",
      "\n",
      "CPU times: user 571 ms, sys: 0 ns, total: 571 ms\n",
      "Wall time: 566 ms\n"
     ]
    }
   ],
   "source": [
    "# Random Forest\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rf_model = RandomForestClassifier()\n",
    "%time _ = train_evaluate(rf_model, features_train, np.array(labels_train), model_name=f\"RFC() n-gram={n}\", n_splits=5, fit_whole_dataset=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "342472e9-348a-4485-b9e1-804d97872b81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split 0 done\n",
      "Split 1 done\n",
      "Split 2 done\n",
      "Split 3 done\n",
      "Split 4 done\n",
      "DTC() n-gram=1: CV-score = 0.935, Variance = 0.0551\n",
      "\n",
      "CPU times: user 98.9 ms, sys: 0 ns, total: 98.9 ms\n",
      "Wall time: 95.8 ms\n"
     ]
    }
   ],
   "source": [
    "# Decision Tree\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "dt_model = DecisionTreeClassifier()\n",
    "%time _ = train_evaluate(dt_model, features_train, np.array(labels_train), model_name=f\"DTC() n-gram={n}\", n_splits=5, fit_whole_dataset=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "a80ad089-08d1-4672-a56d-fed7d1700537",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split 0 done\n",
      "Split 1 done\n",
      "Split 2 done\n",
      "Split 3 done\n",
      "Split 4 done\n",
      "SVC() n-gram=1: CV-score = 0.569, Variance = 0.0398\n",
      "\n",
      "CPU times: user 132 ms, sys: 3.59 ms, total: 136 ms\n",
      "Wall time: 133 ms\n"
     ]
    }
   ],
   "source": [
    "# SVM\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "svc_model = SVC()\n",
    "%time _ = train_evaluate(svc_model, features_train, np.array(labels_train), model_name=f\"SVC() n-gram={n}\", n_splits=5, fit_whole_dataset=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cb4127d-5fb8-4651-ab12-de68f8700e43",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### 2.6 `analyzer='word'` and `n-gram==2` (500 training ex)Â¶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "82c3bdcd-a5b8-42ca-9608-d6e40ae30088",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split 0 done\n",
      "Split 1 done\n",
      "Split 2 done\n",
      "Split 3 done\n",
      "Split 4 done\n",
      "GBC() n-gram=2: CV-score = 0.934, Variance = 0.0401\n",
      "\n",
      "CPU times: user 6.98 s, sys: 0 ns, total: 6.98 s\n",
      "Wall time: 6.97 s\n"
     ]
    }
   ],
   "source": [
    "# Gradient Boosting\n",
    "\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "gb_model = GradientBoostingClassifier()\n",
    "%time _ = train_evaluate(gb_model, features_train, np.array(labels_train), model_name=f\"GBC() n-gram={n}\", n_splits=5, fit_whole_dataset=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "bd6712f3-10a0-4278-808f-49109a38c2d9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split 0 done\n",
      "Split 1 done\n",
      "Split 2 done\n",
      "Split 3 done\n",
      "Split 4 done\n",
      "RFC() n-gram=2: CV-score = 0.930, Variance = 0.0416\n",
      "\n",
      "CPU times: user 557 ms, sys: 294 Âµs, total: 557 ms\n",
      "Wall time: 552 ms\n"
     ]
    }
   ],
   "source": [
    "# Random Forest\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rf_model = RandomForestClassifier()\n",
    "%time _ = train_evaluate(rf_model, features_train, np.array(labels_train), model_name=f\"RFC() n-gram={n}\", n_splits=5, fit_whole_dataset=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "5b9bc986-422c-443e-8544-2b83a96b95c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split 0 done\n",
      "Split 1 done\n",
      "Split 2 done\n",
      "Split 3 done\n",
      "Split 4 done\n",
      "DTC() n-gram=2: CV-score = 0.910, Variance = 0.0353\n",
      "\n",
      "CPU times: user 174 ms, sys: 3.34 ms, total: 178 ms\n",
      "Wall time: 175 ms\n"
     ]
    }
   ],
   "source": [
    "# Decision Tree\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "dt_model = DecisionTreeClassifier()\n",
    "%time _ = train_evaluate(dt_model, features_train, np.array(labels_train), model_name=f\"DTC() n-gram={n}\", n_splits=5, fit_whole_dataset=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "d760b781-5746-4779-a4ef-adc2ecfd61bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split 0 done\n",
      "Split 1 done\n",
      "Split 2 done\n",
      "Split 3 done\n",
      "Split 4 done\n",
      "SVC() n-gram=2: CV-score = 0.500, Variance = 0.0000\n",
      "\n",
      "CPU times: user 173 ms, sys: 2.77 ms, total: 176 ms\n",
      "Wall time: 173 ms\n"
     ]
    }
   ],
   "source": [
    "# SVM\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "svc_model = SVC()\n",
    "%time _ = train_evaluate(svc_model, features_train, np.array(labels_train), model_name=f\"SVC() n-gram={n}\", n_splits=5, fit_whole_dataset=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f05fa59-cd03-44fa-82ae-8b940fde1883",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### 2.7  `analyzer='char'` and `n-gram==2` and `min_leaf==2` (`loss`, `lr`, `n_est`, `crit`, `max_depth` don't improve perf) (500 training ex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "45d8284b-cb3a-4248-ab9c-02702e7aa4e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split 0 done\n",
      "Split 1 done\n",
      "Split 2 done\n",
      "Split 3 done\n",
      "Split 4 done\n",
      "GBC(min_samples_leaf=2, random_state=0, warm_start=True) n-gram=2: CV-score = 1.000, Variance = 0.0000\n",
      "\n",
      "CPU times: user 4.26 s, sys: 190 Âµs, total: 4.26 s\n",
      "Wall time: 4.26 s\n"
     ]
    }
   ],
   "source": [
    "# Gradient Boosting\n",
    "\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "gb_model = GradientBoostingClassifier(min_samples_leaf=2, random_state=0, warm_start=True)\n",
    "%time _ = train_evaluate(gb_model, features_train, np.array(labels_train), model_name=f\"GBC(min_samples_leaf=2, random_state=0, warm_start=True) n-gram={n}\", n_splits=5, fit_whole_dataset=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97881486-d0a7-4f04-9a1b-7bed583ae7a2",
   "metadata": {},
   "source": [
    "### 2.8  `analyzer='char'` and `n-gram==2` and `min_leaf==2` (`loss`, `lr`, `n_est`, `crit`, `max_depth` don't improve perf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "37538675-df46-4eaa-a7e3-ab035aa8c416",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split 0 done\n",
      "Split 1 done\n",
      "Split 2 done\n",
      "Split 3 done\n",
      "Split 4 done\n",
      "GBC(min_s_leaf=2, rs=0, warm_start=True) n-gram=2: CV-score = 0.995, Variance = 0.0085\n",
      "\n",
      "CPU times: user 5min 14s, sys: 1.18 s, total: 5min 15s\n",
      "Wall time: 5min 15s\n"
     ]
    }
   ],
   "source": [
    "# Gradient Boosting\n",
    "\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "gb_model = GradientBoostingClassifier(min_samples_leaf=2, random_state=0, warm_start=True)\n",
    "%time _ = train_evaluate(gb_model, features_train, np.array(labels_train), model_name=f\"GBC(min_s_leaf=2, rs=0, warm_start=True) n-gram={n}\", n_splits=5, fit_whole_dataset=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5c2c764-dc74-4f2e-9617-8a3de8afdb2d",
   "metadata": {},
   "source": [
    "## 3 Prediction on test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6403c32a-8cdc-47c8-b59f-f1d2d2ee382f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48 data/rtf-2017-09/apzvmnmcobbgxqcj.x UnicodeDecodeError\n",
      "74 data/rtf-2017-09/zltktsghvhtowhrs.x UnicodeDecodeError\n",
      "78 data/rtf-2017-09/aagxsnlsqujyoonf.x UnicodeDecodeError\n",
      "122 data/rtf-2017-09/duxajsiflswpxfmw.x UnicodeDecodeError\n",
      "129 data/rtf-2017-09/gwhsyfgsjmhalbvr.x UnicodeDecodeError\n",
      "147 data/rtf-2017-09/rgkcktzjeljqywek.x Refuse to decode: too long\n",
      "150 data/rtf-2017-09/qaiufqizlqdyflor.x Refuse to decode: too long\n",
      "171 data/rtf-2017-09/qqzxuqhsnrxgxqws.x UnicodeDecodeError\n",
      "241 data/rtf-2017-09/agwzmflnvhofyjql.x UnicodeDecodeError\n",
      "252 data/rtf-2017-09/fsptojetbksyxttn.x UnicodeDecodeError\n",
      "286 data/rtf-2017-09/kyhtbqxglbjoogbw.x UnicodeDecodeError\n",
      "319 data/rtf-2017-09/wouvgrffdzymgihn.x UnicodeDecodeError\n",
      "328 data/rtf-2017-09/ywewuzlgvtwcyhag.x UnicodeDecodeError\n",
      "337 data/rtf-2017-09/uvnalkzosteedygr.x UnicodeDecodeError\n",
      "341 data/rtf-2017-09/gkxvvkydpepdfege.x UnicodeDecodeError\n",
      "365 data/rtf-2017-09/ajqyvrgusehfnzjl.x UnicodeDecodeError\n",
      "384 data/rtf-2017-09/ylrpshvilzximekz.x UnicodeDecodeError\n",
      "405 data/rtf-2017-09/nszhppmxmjfmkuey.x UnicodeDecodeError\n",
      "416 data/rtf-2017-09/axwydsszuufnbqfp.x UnicodeDecodeError\n",
      "422 data/rtf-2017-09/rwbmgudqojdmwqkn.x UnicodeDecodeError\n",
      "454 data/rtf-2017-09/rrpezmxgqtlghgmc.x UnicodeDecodeError\n",
      "463 data/rtf-2017-09/etamghqswitoydto.x UnicodeDecodeError\n",
      "467 data/rtf-2017-09/nmxkzebylhabluuw.x UnicodeDecodeError\n",
      "514 data/rtf-2017-09/yniygthvznvgdupr.x UnicodeDecodeError\n",
      "520 data/rtf-2017-09/girxjxjkvpopcpap.x UnicodeDecodeError\n",
      "533 data/rtf-2017-09/zliscugtyrzpndqz.x UnicodeDecodeError\n",
      "538 data/rtf-2017-09/gliwdkbpdrrnasgd.x UnicodeDecodeError\n",
      "560 data/rtf-2017-09/ztczslpzkmdqghcm.x UnicodeDecodeError\n",
      "563 data/rtf-2017-09/tuvecvbklrpdugky.x UnicodeDecodeError\n",
      "605 data/rtf-2017-09/yrqyldzyipmoutjw.x UnicodeDecodeError\n",
      "620 data/rtf-2017-09/tughkkmdmpjdqhvi.x UnicodeDecodeError\n",
      "648 data/rtf-2017-09/wjqkshbnhkhdfwml.x UnicodeDecodeError\n",
      "712 data/rtf-2017-09/rbskmgwismwutvfx.x UnicodeDecodeError\n",
      "730 data/rtf-2017-09/trfpdaedeggkiwjy.x UnicodeDecodeError\n",
      "745 data/rtf-2017-09/pcxmghsvzenexqxc.x UnicodeDecodeError\n",
      "747 data/rtf-2017-09/urwfujvsllrrfrxo.x UnicodeDecodeError\n",
      "764 data/rtf-2017-09/oaeidlgqsxapnxik.x UnicodeDecodeError\n",
      "773 data/rtf-2017-09/uwpfkutlqslvfuvs.x UnicodeDecodeError\n",
      "791 data/rtf-2017-09/vmxsakapaeghnsyf.x UnicodeDecodeError\n",
      "849 data/rtf-2017-09/scyvgmqpxbrygcbl.x UnicodeDecodeError\n",
      "866 data/rtf-2017-09/thjswnyaqpubhryd.x UnicodeDecodeError\n",
      "883 data/rtf-2017-09/njmezjudjrxubweo.x UnicodeDecodeError\n",
      "898 data/rtf-2017-09/yrvriirutosyfkzg.x UnicodeDecodeError\n",
      "906 data/rtf-2017-09/ntovzjtaaopqegcb.x UnicodeDecodeError\n",
      "971 data/rtf-2017-09/dnppuvbgugevofve.x UnicodeDecodeError\n",
      "996 data/rtf-2017-09/jtcrrhjfhamudpcx.x UnicodeDecodeError\n",
      "1029 data/rtf-2017-09/mwvfkktrcxvjwedn.x UnicodeDecodeError\n",
      "1038 data/rtf-2017-09/vmlrdoebbqsmxynn.x UnicodeDecodeError\n",
      "1111 data/rtf-2017-09/uhthlwhomrwmnijw.x UnicodeDecodeError\n",
      "1121 data/rtf-2017-09/xjgvgxecbpcayygx.x UnicodeDecodeError\n",
      "1217 data/rtf-2017-09/kocoyhkojnbyprec.x UnicodeDecodeError\n",
      "1230 data/rtf-2017-09/hvbzknwxrdtcrlcs.x UnicodeDecodeError\n",
      "1303 data/rtf-2017-09/anmuekvxdjdojwyb.x UnicodeDecodeError\n",
      "1311 data/rtf-2017-09/izblovdenddoezwn.x UnicodeDecodeError\n",
      "1361 data/rtf-2017-09/kshgztqpglgdihfm.x UnicodeDecodeError\n",
      "1383 data/rtf-2017-09/zxulyihiqbvhnrus.x UnicodeDecodeError\n",
      "1436 data/rtf-2017-09/oyjfsydfpsduzslc.x UnicodeDecodeError\n",
      "1441 data/rtf-2017-09/hejzarpjlqdbxmqe.x UnicodeDecodeError\n",
      "1450 data/rtf-2017-09/lqrifirpyyvkjgyj.x UnicodeDecodeError\n",
      "1469 data/rtf-2017-09/hujohznovopphmhp.x UnicodeDecodeError\n",
      "1476 data/rtf-2017-09/utavvcobfezmetng.x UnicodeDecodeError\n",
      "1722 data/rtf-2017-09/gngtugspxeigisjw.x UnicodeDecodeError\n",
      "1738 data/rtf-2017-09/sogbcwhsurvfbttc.x UnicodeDecodeError\n",
      "1763 data/rtf-2017-09/ghsnzmaubsusbsdj.x UnicodeDecodeError\n",
      "1786 data/rtf-2017-09/wjaupjeezupbirtu.x UnicodeDecodeError\n",
      "1834 data/rtf-2017-09/zgmawcrrtaeldtsi.x UnicodeDecodeError\n",
      "1841 data/rtf-2017-09/izwzbvwgfszhgxyw.x UnicodeDecodeError\n",
      "Number of non-decoded sequences: 80\n",
      "Percentage of non-decoded sequences: 4.33 %\n"
     ]
    }
   ],
   "source": [
    "##### Get data -> Try decoding it with the second read_rtf_file commented function #####\n",
    "\n",
    "# List of the .rtf filenames of our train or test dataset\n",
    "rtf_filenames_test = zipfile.ZipFile('rtf-test.zip').namelist()\n",
    "# rtf_filenames_test = rtf_filenames_test[:147] + rtf_filenames_test[148:150] + rtf_filenames_test[151:] # files 147 and 150 are pbtq\n",
    "decode = True\n",
    "\n",
    "# labels_test, decoded_indices_test = get_decoded_indices_labels(rtf_filenames_test, decode=decode, train=False)\n",
    "labels_test, decoded_indices_test = [], np.ones(len(rtf_filenames_test))\n",
    "generator_rtf_test = get_rtf_generator(rtf_filenames_test, labels_test, decoded_indices_test, decode=decode, train=False)\n",
    "\n",
    "\n",
    "##### Extract features #####\n",
    "\n",
    "features_test = vectorizer.transform(generator_rtf_test)\n",
    "features_test = features_test.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "83220b90-f54a-4fbf-98df-0c4c61f70eb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "features_test.shape = (1767, 47298)\n"
     ]
    }
   ],
   "source": [
    "##### Save/load data #####\n",
    "\n",
    "# np.save('features_test_n2_df1.npy', features_test)\n",
    "features_test = np.load('features_test_n2_df1.npy')\n",
    "print(f'{features_test.shape = }')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "feb5f0e6-9ac1-4e17-a120-5ab866d107ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Predict of test dataset with SVC #####\n",
    "\n",
    "X_test = features_test\n",
    "y_pred = gb_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "70cf4d3a-5a14-41e2-9b09-962c82ee1e4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of our submission: 1847 | Length of zip file: 1847\n"
     ]
    }
   ],
   "source": [
    "submission = []\n",
    "i_corr = 0\n",
    "# Write the prediction as expected output\n",
    "for i, filename in enumerate(rtf_filenames_test):\n",
    "    if decoded_indices_test[i]:\n",
    "        submission += [filename + ';' + y_pred[i-i_corr].astype(str)]\n",
    "    else: # if email hasn't been decoded and thus predicted, we randomly choose its class / assign it to class 1\n",
    "        # submission += [filename + ';' + str(np.random.randint(2))]\n",
    "        submission += [filename + ';' + str(1)]\n",
    "        i_corr += 1\n",
    "print(f'Length of our submission: {len(submission)} | Length of zip file: {len(rtf_filenames_test)}')\n",
    "# Save the output as a text file\n",
    "np.savetxt('output_gb_n2_df1.csv', np.array(submission), fmt='%s', delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9468b690-85e0-4f1f-8fa6-233a29777202",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
