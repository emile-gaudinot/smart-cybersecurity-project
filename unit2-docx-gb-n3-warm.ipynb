{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8a2ec8d5-d755-4140-a96a-13c90f42ffcf",
   "metadata": {},
   "source": [
    "# Docx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "e6f9f1b0-4540-48af-aa39-ab64796a5b9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import psutil\n",
    "import zipfile\n",
    "import chardet\n",
    "import numpy as np\n",
    "import xml.etree.ElementTree as ET\n",
    "import pickle\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import accuracy_score, balanced_accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b011708a-0f70-4de2-8d9e-bb87a945bbe0",
   "metadata": {},
   "source": [
    "## 0 RAM usage monitoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "cf26fe57-f7fe-4f73-ba01-557aaa5b5085",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current RAM usage: 0.37 / 24 GB (1.5 %)\n"
     ]
    }
   ],
   "source": [
    "def get_ram_usage():\n",
    "    memory_info = psutil.Process().memory_info()\n",
    "    return memory_info.rss / (1024 * 1024 * 1024)  # Resident Set Size (RSS) in bytes\n",
    "\n",
    "print(f\"Current RAM usage: {get_ram_usage():.2f} / 24 GB ({get_ram_usage()/24*100:.1f} %)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52ba6aca-74ae-4485-a3b4-56aa42ec48b5",
   "metadata": {},
   "source": [
    "## 1 Process data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61e92952-e20f-4ab1-99f9-8cf0e4e2b0da",
   "metadata": {},
   "source": [
    "### 1.1 Get files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "e75594ea-2011-4c51-ab4e-166e0bde4a7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_parsed_docx(docx_filenames_train, labels_train, decoded_index_train, train=True):\n",
    "    print('Number of training .docx:', len(docx_filenames_train))\n",
    "    errors_train = 0\n",
    "    \n",
    "    for i, docxname in enumerate(docx_filenames_train):\n",
    "        try:\n",
    "            # Get one document, that is a zip archive with many XML files, and its label\n",
    "            docx = zipfile.ZipFile(docxname)\n",
    "            docxroot = ET.Element('root')\n",
    "            for xmlname in docx.namelist():\n",
    "                if xmlname[-3:] == 'xml':\n",
    "                    # For all the .xml files in ou docx, parse it and stores the parsed tree in docxroot, which will be an ElementTree\n",
    "                    xmlfile = docx.read(xmlname)\n",
    "                    encoding = chardet.detect(xmlfile)[\"encoding\"]\n",
    "                    xmlfile = xmlfile.decode(encoding if encoding != None else 'utf-8')\n",
    "                    tree = ET.fromstring(xmlfile)\n",
    "                    docxroot.append(tree)\n",
    "            # Now each docx has to be one ElementTree, whose children are each single .xml included in the .docx \n",
    "            if train:\n",
    "                labels_train += [int(docxname[-1])]\n",
    "            docxtree = ET.ElementTree(docxroot)\n",
    "            yield ET.tostring(docxtree.getroot(), encoding='unicode')\n",
    "        except zipfile.BadZipFile as e:\n",
    "            # Handle bad zipfile error\n",
    "            print(i, docxname, 'zipfile.BadZipFile')\n",
    "            errors_train += 1\n",
    "            decoded_index_train[i] = 0\n",
    "            continue\n",
    "        except UnicodeDecodeError:\n",
    "            # Deal with decoding error\n",
    "            print(i, docxname, 'UnicodeDecodeError')\n",
    "            errors_train += 1\n",
    "            decoded_index_train[i] = 0\n",
    "            continue\n",
    "        except ET.ParseError:\n",
    "            # Deal with parsing error\n",
    "            print(i, docxname, 'ET.ParseError')\n",
    "            errors_train += 1\n",
    "            decoded_index_train[i] = 0\n",
    "            continue\n",
    "    # Print error number + return parsed list\n",
    "    print('Number of non-decoded sequences:', errors_train)\n",
    "    print('Percentage of non-decoded sequences:', round(errors_train/len(docx_filenames_train)*100, 2), '%')\n",
    "\n",
    "# docx_filenames_train = zipfile.ZipFile('docx-train.zip').namelist()[:-1]\n",
    "# # docx_filenames_train = zipfile.ZipFile('docx-train.zip').namelist()[:100]\n",
    "# labels_train, decoded_index_train = [], np.ones(len(docx_filenames_train))\n",
    "# docx_generator_train = get_parsed_docx(docx_filenames_train, labels_train, decoded_index_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eb814e1-d7bd-4599-a04b-241335d553a9",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### 1.2 Save and load parsed docx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1fc83ca6-6b17-456a-baaf-b9cdafcbb702",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'docx-train-3000.pkl'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 16\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m array\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# save_data('docx-train-3000.pkl', parsed_docx_train)\u001b[39;00m\n\u001b[0;32m---> 16\u001b[0m parsed_docx_train \u001b[38;5;241m=\u001b[39m \u001b[43mload_data\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdocx-train-3000.pkl\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# save_data('labels-train-3000.pkl', labels_train)\u001b[39;00m\n\u001b[1;32m     18\u001b[0m labels_train \u001b[38;5;241m=\u001b[39m load_data(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabels-train-3000.pkl\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[0;32mIn[4], line 11\u001b[0m, in \u001b[0;36mload_data\u001b[0;34m(filename)\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_data\u001b[39m(filename):\n\u001b[1;32m      8\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;124;03m    LOAD the elements from the file using pickle\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 11\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m file:\n\u001b[1;32m     12\u001b[0m         array \u001b[38;5;241m=\u001b[39m pickle\u001b[38;5;241m.\u001b[39mload(file)\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m array\n",
      "File \u001b[0;32m~/env/lib/python3.11/site-packages/IPython/core/interactiveshell.py:286\u001b[0m, in \u001b[0;36m_modified_open\u001b[0;34m(file, *args, **kwargs)\u001b[0m\n\u001b[1;32m    279\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[1;32m    280\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    281\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    282\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    283\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    284\u001b[0m     )\n\u001b[0;32m--> 286\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'docx-train-3000.pkl'"
     ]
    }
   ],
   "source": [
    "def save_data(filename, array):\n",
    "    \"\"\"\n",
    "    SAVE the elements to a file using pickle\n",
    "    \"\"\"\n",
    "    with open(filename, 'wb') as file:\n",
    "        pickle.dump(array, file)\n",
    "def load_data(filename):\n",
    "    \"\"\"\n",
    "    LOAD the elements from the file using pickle\n",
    "    \"\"\"\n",
    "    with open(filename, 'rb') as file:\n",
    "        array = pickle.load(file)\n",
    "    return array\n",
    "\n",
    "# save_data('docx-train-3000.pkl', parsed_docx_train)\n",
    "# parsed_docx_train = load_data('docx-train-3000.pkl')\n",
    "# save_data('labels-train-3000.pkl', labels_train)\n",
    "# labels_train = load_data('labels-train-3000.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58bdf56c-6217-4a0e-a1e6-19b0be3054fd",
   "metadata": {},
   "source": [
    "### 1.3 Feature extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e0d70e5-16a0-42a9-b1ed-988b40926f70",
   "metadata": {},
   "source": [
    "Overview of the attributes of an `Element`:\\\n",
    "`tag`: The tag of the element.\\\n",
    "`attrib`: A dictionary containing the element's attributes.\\\n",
    "`text`: The text content of the element.\\\n",
    "`iter()`: An iterator that generates all the subelements of the element.\\\n",
    "`find()`: Finds the first subelement with the given tag.\\\n",
    "`findall()`: Finds all subelements with the given tag."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "5e8da1f3-06c6-435e-9eb1-a1aa144241aa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training .docx: 6301\n",
      "224 data/docx-2017-01/uclvhtuckhtprhgn.1 zipfile.BadZipFile\n",
      "294 data/docx-2017-01/lbfoxladmzymqvfj.1 zipfile.BadZipFile\n",
      "337 data/docx-2017-01/vqesbgqyinhhnvfk.1 zipfile.BadZipFile\n",
      "404 data/docx-2016-07/frriizhcbhhatlxq.0 UnicodeDecodeError\n",
      "518 data/docx-2017-01/btnagxosptxudviw.1 zipfile.BadZipFile\n",
      "929 data/docx-2017-01/wahhucrnhhaownvm.1 zipfile.BadZipFile\n",
      "1186 data/docx-2016-07/sevvokgiaznvbfub.1 ET.ParseError\n",
      "1256 data/docx-2017-01/thkjtswtmjlvlrcr.1 zipfile.BadZipFile\n",
      "1337 data/docx-2016-07/pmwdrnsvonimrbyq.0 UnicodeDecodeError\n",
      "1349 data/docx-2016-07/ijccvegzcbnbfjwo.0 UnicodeDecodeError\n",
      "1657 data/docx-2017-01/uugkgbullwpupcox.1 zipfile.BadZipFile\n",
      "1972 data/docx-2017-01/vvkdmlotszdqjfoz.1 zipfile.BadZipFile\n",
      "2030 data/docx-2017-01/gzoyyidmbehydfpp.1 zipfile.BadZipFile\n",
      "2038 data/docx-2016-07/vbntkivddaodqttn.0 zipfile.BadZipFile\n",
      "2345 data/docx-2017-01/ckfayemcuarwfnlm.1 zipfile.BadZipFile\n",
      "2357 data/docx-2016-07/uhssqmrzgeynsmxp.0 UnicodeDecodeError\n",
      "2428 data/docx-2017-01/byohissewbdjpuxz.0 UnicodeDecodeError\n",
      "3093 data/docx-2017-01/wogqvtdsemrsraie.1 zipfile.BadZipFile\n",
      "3165 data/docx-2017-01/idcxhsgpcthznlxy.1 zipfile.BadZipFile\n",
      "3301 data/docx-2017-01/bnnqziiqewifdzao.1 zipfile.BadZipFile\n",
      "3468 data/docx-2017-01/sqobpywxucitpylx.1 zipfile.BadZipFile\n",
      "3494 data/docx-2017-01/qiflytovbwgatkeq.1 zipfile.BadZipFile\n",
      "3714 data/docx-2017-01/cjskpuzbcmjrxcin.1 zipfile.BadZipFile\n",
      "3727 data/docx-2017-01/rrcwxpnhznvjupnk.1 zipfile.BadZipFile\n",
      "3890 data/docx-2017-01/wqiunfptqinexywq.1 zipfile.BadZipFile\n",
      "4080 data/docx-2017-01/dzusjneusiocfbne.0 ET.ParseError\n",
      "4317 data/docx-2016-07/xkljurixdnbvxlic.0 ET.ParseError\n",
      "4850 data/docx-2016-07/nygpvxgtavxbvqcg.0 UnicodeDecodeError\n",
      "5022 data/docx-2017-01/coipwiklzddagcxl.1 zipfile.BadZipFile\n",
      "5150 data/docx-2016-07/qyvpdtmuncsykmcy.1 zipfile.BadZipFile\n",
      "5259 data/docx-2016-07/uupjhcjcqqtajsuz.0 UnicodeDecodeError\n",
      "5274 data/docx-2016-07/ondrcyxpiaycmrvh.0 UnicodeDecodeError\n",
      "5450 data/docx-2016-07/gvzuiettdplxtbtk.0 UnicodeDecodeError\n",
      "5474 data/docx-2017-01/ipmijeioquyhduol.1 zipfile.BadZipFile\n",
      "5646 data/docx-2017-01/blxkipgwbwuepbpd.1 zipfile.BadZipFile\n",
      "5960 data/docx-2016-07/kkmsuiafpzcfmxbi.0 UnicodeDecodeError\n",
      "6292 data/docx-2016-07/ouruvhbbegsrsmcv.0 UnicodeDecodeError\n",
      "Number of non-decoded sequences: 37\n",
      "Percentage of non-decoded sequences: 0.59 %\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(6264, 105814)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n = 3\n",
    "vectorizer = CountVectorizer(strip_accents='unicode', lowercase=True, ngram_range=(n,n), analyzer='char', min_df=5)\n",
    "features_train = vectorizer.fit_transform(docx_generator_train)\n",
    "features_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3299d6d3-37ea-4c8e-9da8-0681294d29a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.save('features_train_n3_bis.npy', features_train)\n",
    "# features_train = np.load('features_train_n3.npy')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cab5fddf-5ce3-4d4f-b7c1-f118955ead65",
   "metadata": {},
   "source": [
    "## 2 Train models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b62b7097-bd7c-494c-9928-0cc451f3e15f",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### 2.0 Train any model using 5-fold cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b4aa8647-8761-4739-b7ee-2e13da80eff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_evaluate(model, features, labels, model_name='base-model', n_splits=5, fit_whole_dataset=True):\n",
    "    # K-Fold CV\n",
    "    kf = KFold(n_splits=n_splits, shuffle=True, random_state=0)\n",
    "    accuracy_list, var_list = [], []\n",
    "    split = 0\n",
    "    \n",
    "    # Training + metrics\n",
    "    for idx_train, idx_eval in kf.split(features):\n",
    "        X_train, X_eval, y_train, y_eval = features[idx_train], features[idx_eval], labels[idx_train], labels[idx_eval]\n",
    "        # Fit the model\n",
    "        model.fit(X_train, y_train)\n",
    "        # Predict\n",
    "        y_pred = model.predict(X_eval)\n",
    "        # Evaluate (accuracy and F1-score)\n",
    "        acc = balanced_accuracy_score(y_eval, y_pred)\n",
    "        accuracy_list += [acc*len(y_eval)]\n",
    "        var_list += [acc]\n",
    "        print(f'Split {split} done')\n",
    "        split += 1\n",
    "        \n",
    "    # Compute CV_score\n",
    "    cvscore = sum(accuracy_list)/len(labels)\n",
    "    variance = np.std(var_list)\n",
    "    print(f'CV-score of {model_name}: CV-score = {cvscore:.3f}, Variance = {variance:.4f}\\n')\n",
    "\n",
    "    # Train the model on the whole Train dataset\n",
    "    if fit_whole_dataset:\n",
    "        model.fit(features, labels)\n",
    "    \n",
    "    return cvscore, variance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82c85827-038e-4212-b265-60a66013aecf",
   "metadata": {},
   "source": [
    "### 2.1 XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b1060a99-bc05-4bb9-9d15-7b6a2631f978",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split 0 done\n",
      "Split 1 done\n",
      "Split 2 done\n",
      "Split 3 done\n",
      "Split 4 done\n",
      "CV-score of GBC(): CV-score = 0.992, Variance = 0.0027\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "gb_model = GradientBoostingClassifier()\n",
    "%time _ = train_evaluate(gb_model, features_train, np.array(labels_train), model_name=\"GBC()\", n_splits=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "89657dc0-cd7e-477d-8820-0f3b6c13994e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split 0 done\n",
      "Split 1 done\n",
      "Split 2 done\n",
      "Split 3 done\n",
      "Split 4 done\n",
      "CV-score of GBC(warm_start=True): CV-score = 0.999, Variance = 0.0024\n",
      "\n",
      "CPU times: user 5min 35s, sys: 6.32 s, total: 5min 42s\n",
      "Wall time: 5min 42s\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "gb_model = GradientBoostingClassifier(random_state=0, warm_start=True)\n",
    "%time _ = train_evaluate(gb_model, features_train, np.array(labels_train), model_name=\"GBC(warm_start=True)\", n_splits=5, fit_whole_dataset=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0974c61-9ce3-45f2-8ee5-646239c0f29d",
   "metadata": {},
   "source": [
    "## 3 Predict on test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "152b5d3f-37a4-44c6-8b2d-05368164e010",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "docx_filenames_test = zipfile.ZipFile('docx-test.zip').namelist()[:-1]\n",
    "# docx_filenames_test = zipfile.ZipFile('docx-test.zip').namelist()[:100]\n",
    "labels_test, decoded_index_train = [], np.ones(len(docx_filenames_test))\n",
    "docx_generator_test = get_parsed_docx(docx_filenames_test, labels_test, decoded_index_train, train=False)\n",
    "decoded_index_test = decoded_index_train.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "d77aa97b-d8a5-4d04-b7f2-b39399230562",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training .docx: 2968\n",
      "200 data/docx-2017-09/hjxzjigfrcpqkmwc.x zipfile.BadZipFile\n",
      "366 data/docx-2017-09/klecwwdgkwzyqudm.x UnicodeDecodeError\n",
      "516 data/docx-2017-09/lyhfmwjzqltgztwi.x zipfile.BadZipFile\n",
      "566 data/docx-2017-09/qgrzabkbbcbswbfv.x zipfile.BadZipFile\n",
      "568 data/docx-2017-09/quzappguarpuodwu.x zipfile.BadZipFile\n",
      "602 data/docx-2017-09/qwxbljpxglasxfnp.x zipfile.BadZipFile\n",
      "854 data/docx-2017-09/lmawooifkluqjzcl.x UnicodeDecodeError\n",
      "868 data/docx-2017-09/muexzdvjdtzkbqrb.x zipfile.BadZipFile\n",
      "904 data/docx-2017-09/jjizjcnumcgnrvda.x zipfile.BadZipFile\n",
      "937 data/docx-2017-09/amvmomyzjtsimsyb.x zipfile.BadZipFile\n",
      "941 data/docx-2017-09/qhryecbrcxcmufpp.x zipfile.BadZipFile\n",
      "1009 data/docx-2017-09/dmpiyfxfiytsafsf.x zipfile.BadZipFile\n",
      "1207 data/docx-2017-09/zwsvmqtrunodfqxr.x UnicodeDecodeError\n",
      "1233 data/docx-2017-09/tflajjtenmjesywc.x zipfile.BadZipFile\n",
      "1340 data/docx-2017-09/qpfqebykcfediaja.x UnicodeDecodeError\n",
      "1408 data/docx-2017-09/fvscslimbxolnxdb.x zipfile.BadZipFile\n",
      "1484 data/docx-2017-09/saayhyhhwfxjdjax.x UnicodeDecodeError\n",
      "1485 data/docx-2017-09/olgvgrcepwlxofnu.x zipfile.BadZipFile\n",
      "1786 data/docx-2017-09/jmncpfnwdvcobxcp.x UnicodeDecodeError\n",
      "2116 data/docx-2017-09/yoqlnghmxawaqone.x UnicodeDecodeError\n",
      "2217 data/docx-2017-09/ljrwbinbzdljimwh.x zipfile.BadZipFile\n",
      "2317 data/docx-2017-09/wzxafqrhereksosv.x zipfile.BadZipFile\n",
      "2397 data/docx-2017-09/qdztzoikioaoozli.x zipfile.BadZipFile\n",
      "2607 data/docx-2017-09/mkrzfjtlukmwclzb.x zipfile.BadZipFile\n",
      "2823 data/docx-2017-09/jagepirzopbouuzf.x zipfile.BadZipFile\n",
      "Number of non-decoded sequences: 25\n",
      "Percentage of non-decoded sequences: 0.84 %\n"
     ]
    }
   ],
   "source": [
    "# docx_string_test = [ET.tostring(docx.getroot(), encoding='unicode') for docx in parsed_docx_test]\n",
    "features_test = vectorizer.transform(docx_generator_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "1d26352b-1475-42e6-b40c-9b3e7e3f0c52",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Predict of test dataset with GB #####\n",
    "\n",
    "X_test = features_test\n",
    "y_pred = gb_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "3f7876d8-705c-4cd6-9b3b-103e364d968b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of our submission: 2968 | Length of zip file: 2968\n"
     ]
    }
   ],
   "source": [
    "submission = []\n",
    "i_corr = 0\n",
    "# Write the prediction as expected output\n",
    "for i, filename in enumerate(docx_filenames_test):\n",
    "    if decoded_index_test[i]:\n",
    "    # if decoded_indices_test[i]:\n",
    "        submission += [filename + ';' + y_pred[i-i_corr].astype(str)]\n",
    "    else: # if email hasn't been decoded and thus predicted, we randomly choose its class\n",
    "        submission += [filename + ';' + str(1)]\n",
    "        i_corr += 1\n",
    "print(f'Length of our submission: {len(submission)} | Length of zip file: {len(docx_filenames_test)}')\n",
    "# Save the output as a text file\n",
    "np.savetxt('output_docx_gb_n3_warm.csv', np.array(submission), fmt='%s', delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c6a9028-ec17-425e-a02e-6feeb46f0b43",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
