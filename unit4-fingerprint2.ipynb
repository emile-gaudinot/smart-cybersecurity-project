{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0ae2dfd9-163a-4af7-91af-01fdd1036d7f",
   "metadata": {},
   "source": [
    "# Fingerprinting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9f6d1120-a020-4a04-83e8-efa93491b986",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "\n",
    "import psutil\n",
    "import numpy as np\n",
    "import pyshark\n",
    "import tarfile\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "from collections import Counter\n",
    "import json\n",
    "from scipy.stats import entropy\n",
    "import copy\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "from sklearn.svm import OneClassSVM\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "from sklearn.cluster import DBSCAN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d64ee4d1-16c0-4237-a52c-fe9b1406669c",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## 0 Current RAM usage monitoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "02a8dd03-b96f-4c49-830f-e64b0a0558a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current RAM usage: 0.15 / 24 GB (0.6 %)\n"
     ]
    }
   ],
   "source": [
    "class RamUsage:\n",
    "    def __init__(self):\n",
    "        self.memory_info: float\n",
    "        \n",
    "    def __call__(self):\n",
    "        \"\"\"\n",
    "        Return Occupied RAM in GiB\n",
    "        \"\"\"\n",
    "        self.memory_info = psutil.Process().memory_info().rss / (1024 * 1024 * 1024)\n",
    "        return self.memory_info\n",
    "\n",
    "    def __str__(self):\n",
    "        _ = self.__call__()\n",
    "        return f\"Current RAM usage: {self.memory_info:.2f} / 24 GB ({self.memory_info/24*100:.1f} %)\"\n",
    "ram = RamUsage()\n",
    "print(ram)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6be212c-f081-43dc-bd46-b46024462c3d",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## 1. Data loading and feature extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f6136762-cc14-4e28-b3ba-c82dfb84272d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training example: 590\n",
      "Number of test example: 591\n",
      "Current RAM usage: 0.15 / 24 GB (0.6 %)\n"
     ]
    }
   ],
   "source": [
    "class TorFiles:\n",
    "    def __init__(self, file_directory_train: str, file_directory_test: str):\n",
    "        self.file_directory_train = file_directory_train\n",
    "        self.file_directory_test = file_directory_test\n",
    "        self.filenames_train: list\n",
    "        self.filenames_test: list\n",
    "        self.nb_file_train: int\n",
    "        self.nb_file_test: int\n",
    "        self.single_cap_features: dict\n",
    "        self.all_cap_features_train: dict\n",
    "        self.all_cap_features_test: dict\n",
    "        self.final_features: np.ndarray\n",
    "\n",
    "    def get_filenames(self, train=True):\n",
    "        if train:\n",
    "            with tarfile.open(self.file_directory_train, 'r:gz') as tar:\n",
    "                filenames = tar.getnames()\n",
    "            return filenames\n",
    "        else:\n",
    "            with tarfile.open(self.file_directory_test, 'r:gz') as tar:\n",
    "                filenames = tar.getnames()\n",
    "                if filenames[0] == 'test_data':\n",
    "                    filenames = filenames[1:]\n",
    "            return filenames\n",
    "\n",
    "    def __call__(self, mode: str = 'train'):\n",
    "        self.filenames_train = self.get_filenames(train=True)\n",
    "        self.filenames_test = self.get_filenames(train=False)\n",
    "        self.nb_file_train = len(self.filenames_train)\n",
    "        self.nb_file_test = len(self.filenames_test)\n",
    "            \n",
    "    def __str__(self):\n",
    "        return f\"Number of training example: {self.nb_file_train}\\nNumber of test example: {self.nb_file_test}\\n{ram.__str__()}\"\n",
    "\n",
    "    def init_cap_features(self, single=True, train=True):\n",
    "        \"\"\"\n",
    "        Initialize the features dictionary 'self.single_cap_features' if single=True, 'self.all_cap_features(train/test)' if single=False\n",
    "        \"\"\"\n",
    "        keys = [\n",
    "            'src_ip', 'dst_ip', 'src_port', 'dst_port', 'transport_protocol', 'application_protocol', 'packet_length_mean', 'packet_length_std', \n",
    "            'packet_length_entropy', 'payload_length_mean', 'payload_length_std', 'payload_length_entropy', 'tcp_flags', 'ip_options', 'dns_query', \n",
    "            'dns_response', 'http_method', 'http_status_code', 'http_user_agent', 'http_host', 'tls_handshake', 'inter_arrival_time_mean', \n",
    "            'inter_arrival_time_std', 'window_size_mean', 'window_size_std', 'window_size_entropy', 'rtt_mean', 'rtt_std', 'jitter_mean', 'jitter_std', 'packet_loss',\n",
    "            'retransmission', 'syn_ack_flags', 'icmp_type', 'icmp_code', \n",
    "            'dns_query_type', 'dns_response_code', 'http_content_type', 'http_content_length_mean', 'http_content_length_std', 'tls_version', \n",
    "            'tls_cipher_suite', 'ip_ttl_mean', 'ip_ttl_std', 'ip_ttl_entropy', 'ip_fragmentation', 'tcp_sequence_number_mean', \n",
    "            'tcp_sequence_number_std', 'tcp_acknowledgment_number_mean', 'tcp_acknowledgment_number_std', 'udp_checksum_mean', 'udp_checksum_std', \n",
    "            'icmp_checksum_mean', 'icmp_checksum_std',\n",
    "        ]\n",
    "        values_single = [[] for i in range(len(keys))]\n",
    "        values_all = [Counter({}) for i in range(len(keys))]\n",
    "        if train:\n",
    "            if single:\n",
    "                self.single_cap_features = dict(zip(keys, values_single))\n",
    "            else:\n",
    "                self.all_cap_features_train = dict(zip(keys, values_all))\n",
    "        else:\n",
    "            if single:\n",
    "                self.single_cap_features = dict(zip(keys, values_single))\n",
    "            else:\n",
    "                self.all_cap_features_test = dict(zip(keys, values_all))\n",
    "        \n",
    "    def add_packet_features(self, packet, previous_timestamp=None):\n",
    "        \"\"\"\n",
    "        For a given packet, append the extracted features (1 word/int per feature) to the lists in the dict 'single_packet_features'\n",
    "        \"\"\"\n",
    "        single_packet_features = {\n",
    "            'src_ip': packet.ip.src if 'IP' in packet else '', # BoW (with the whole string, default option)\n",
    "            'dst_ip': packet.ip.dst if 'IP' in packet else '', # BoW\n",
    "            'src_port': packet.tcp.srcport if 'TCP' in packet else (packet.udp.srcport if 'UDP' in packet else ''), # BoW\n",
    "            'dst_port': packet.tcp.dstport if 'TCP' in packet else (packet.udp.dstport if 'UDP' in packet else ''), # BoW\n",
    "            'transport_protocol': packet.transport_layer if 'IP' in packet else '', # BoW\n",
    "            'application_protocol': packet.highest_layer if 'IP' in packet else '', # BoW\n",
    "            'packet_length_mean': int(packet.length), # Mean + distribution entropy\n",
    "            'packet_length_std': int(packet.length),\n",
    "            'packet_length_entropy': int(packet.length),\n",
    "            'payload_length_mean': len(packet) if hasattr(packet, 'data') else 0, # Mean + distribution entropy\n",
    "            'payload_length_std': len(packet) if hasattr(packet, 'data') else 0,\n",
    "            'payload_length_entropy': len(packet) if hasattr(packet, 'data') else 0,\n",
    "            'tcp_flags': packet.tcp.flags if 'TCP' in packet else '', # BoW\n",
    "            'ip_options': 'True' if 'IP' in packet and hasattr(packet.ip, 'options') else 'False', #None if not  else None), # BoW\n",
    "            'dns_query': packet.dns.qry_name if 'DNS' in packet else '', # BoW\n",
    "            'dns_response': packet.dns.resp_name if 'DNS' in packet else '', # BoW\n",
    "            'http_method': packet.http.request_method if 'HTTP' in packet else '', # BoW\n",
    "            'http_status_code': packet.http.response_code if 'HTTP' in packet else '', # BoW\n",
    "            'http_user_agent': packet.http.user_agent if 'HTTP' in packet else '', # BoW with 'word' and n=3?n=10?\n",
    "            'http_host': packet.http.host if 'HTTP' in packet else '', # BoW\n",
    "            'tls_handshake': packet.tls.handshake if 'TLS' in packet and hasattr(packet.tls, 'handshake') else '', # BoW\n",
    "            'tls_certificates': packet.tls.certificates if 'TLS' in packet and hasattr(packet.tls, 'certificates') else '', # BoW\n",
    "            'inter_arrival_time_mean': float(packet.sniff_timestamp) - float(previous_timestamp) if previous_timestamp else 0, # Mean + distribution entropy\n",
    "            'inter_arrival_time_std': float(packet.sniff_timestamp) - float(previous_timestamp) if previous_timestamp else 0,\n",
    "            # Mistral's new features\n",
    "            'window_size_mean': packet.tcp.window_size if 'TCP' in packet and hasattr(packet.tcp, 'window_size') else '',\n",
    "            'window_size_std': packet.tcp.window_size if 'TCP' in packet and hasattr(packet.tcp, 'window_size') else '',\n",
    "            'window_size_entropy': packet.tcp.window_size if 'TCP' in packet and hasattr(packet.tcp, 'window_size') else '',\n",
    "            'rtt_mean': packet.tcp.analysis_ack_rtt if 'TCP' in packet and hasattr(packet.tcp, 'analysis_ack_rtt') else '',\n",
    "            'rtt_std': packet.tcp.analysis_ack_rtt if 'TCP' in packet and hasattr(packet.tcp, 'analysis_ack_rtt') else '',\n",
    "            'jitter_mean': packet.tcp.analysis_ack_rtt_var if 'TCP' in packet and hasattr(packet.tcp, 'analysis_ack_rtt_var') else '',\n",
    "            'jitter_std': packet.tcp.analysis_ack_rtt_var if 'TCP' in packet and hasattr(packet.tcp, 'analysis_ack_rtt_var') else '',\n",
    "            'packet_loss': 'True' if 'TCP' in packet and hasattr(packet.tcp, 'analysis_lost_segment') else 'False',\n",
    "            'retransmission': 'True' if 'TCP' in packet and hasattr(packet.tcp, 'analysis_retransmission') else 'False',\n",
    "            'syn_ack_flags': packet.tcp.flags_syn if 'TCP' in packet and hasattr(packet.tcp, 'flags_syn') else '',\n",
    "            'icmp_type': packet.icmp.type if 'ICMP' in packet and hasattr(packet.icmp, 'type') else '',\n",
    "            'icmp_code': packet.icmp.code if 'ICMP' in packet and hasattr(packet.icmp, 'code') else '',\n",
    "            'dns_query_type': packet.dns.qry_type if 'DNS' in packet and hasattr(packet.dns, 'qry_type') else '',\n",
    "            'dns_response_code': packet.dns.resp_code if 'DNS' in packet and hasattr(packet.dns, 'resp_code') else '',\n",
    "            'http_content_type': packet.http.content_type if 'HTTP' in packet and hasattr(packet.http, 'content_type') else '',\n",
    "            'http_content_length_mean': packet.http.content_length if 'HTTP' in packet and hasattr(packet.http, 'content_length') else '',\n",
    "            'http_content_length_std': packet.http.content_length if 'HTTP' in packet and hasattr(packet.http, 'content_length') else '',\n",
    "            'tls_version': packet.tls.version if 'TLS' in packet and hasattr(packet.tls, 'version') else '',\n",
    "            'tls_cipher_suite': packet.tls.cipher_suite if 'TLS' in packet and hasattr(packet.tls, 'cipher_suite') else '',\n",
    "            'ip_ttl_mean': packet.ip.ttl if 'IP' in packet and hasattr(packet.ip, 'ttl') else '',\n",
    "            'ip_ttl_std': packet.ip.ttl if 'IP' in packet and hasattr(packet.ip, 'ttl') else '',\n",
    "            'ip_ttl_entropy': packet.ip.ttl if 'IP' in packet and hasattr(packet.ip, 'ttl') else '',\n",
    "            'ip_fragmentation': packet.ip.frag_offset if 'IP' in packet and hasattr(packet.ip, 'frag_offset') else '',\n",
    "            'tcp_sequence_number_mean': packet.tcp.seq if 'TCP' in packet and hasattr(packet.tcp, 'seq') else '',\n",
    "            'tcp_sequence_number_std': packet.tcp.seq if 'TCP' in packet and hasattr(packet.tcp, 'seq') else '',\n",
    "            'tcp_acknowledgment_number_mean': packet.tcp.ack if 'TCP' in packet and hasattr(packet.tcp, 'ack') else '',\n",
    "            'tcp_acknowledgment_number_std': packet.tcp.ack if 'TCP' in packet and hasattr(packet.tcp, 'ack') else '',\n",
    "            'udp_checksum_mean': packet.udp.checksum if 'UDP' in packet and hasattr(packet.udp, 'checksum') else '',\n",
    "            'udp_checksum_std': packet.udp.checksum if 'UDP' in packet and hasattr(packet.udp, 'checksum') else '',\n",
    "            'icmp_checksum_mean': packet.icmp.checksum if 'ICMP' in packet and hasattr(packet.icmp, 'checksum') else '',\n",
    "            'icmp_checksum_std': packet.icmp.checksum if 'ICMP' in packet and hasattr(packet.icmp, 'checksum') else '',\n",
    "        }\n",
    "        for key in self.single_cap_features.keys():\n",
    "            self.single_cap_features[key].append(single_packet_features[key])\n",
    "\n",
    "    def read_single_pcap(self, filename, train=True):\n",
    "        \"\"\"\n",
    "        Compute, for the capture associated with a filename, a dict of Counter - each Counter corresponds to a feature e.g. 'src_ip'\n",
    "        Each Counter gives the number of time each feature has been found in the packets, e.g. '192.168.1.0': 400\n",
    "        \"\"\"\n",
    "        capture = pyshark.FileCapture(filename)\n",
    "        # Initialize an empty dict for self.single_cap_features\n",
    "        self.init_cap_features(single=True, train=train)\n",
    "        previous_timestamp = None\n",
    "        for i, packet in enumerate(capture):\n",
    "            # Get features for a single packet\n",
    "            self.add_packet_features(packet, previous_timestamp)\n",
    "            previous_timestamp = packet.sniff_timestamp\n",
    "        # Now we have features for all the packets of a single capture. Let's do a bit of post-processing\n",
    "        self.single_cap_features[\"packet_length_max_value\"] = max(self.single_cap_features[\"packet_length_mean\"])\n",
    "        self.single_cap_features[\"flow_length_value\"] = sum(self.single_cap_features[\"packet_length_mean\"])\n",
    "        self.single_cap_features[\"packet_count_value\"] = i\n",
    "        self.single_cap_features[\"flow_time_value\"] = sum(self.single_cap_features[\"inter_arrival_time_mean\"])\n",
    "        # Post-processing of mean/std/entropy other intersting features\n",
    "        for key in self.single_cap_features.keys():\n",
    "            if key == 'retransmission' and (np.array(self.single_cap_features[key]) == 'True').any():\n",
    "                print(f\"One {key} here.\")\n",
    "            if key == 'packet_loss' and (np.array(self.single_cap_features[key]) == 'True').any():\n",
    "                print(f\"One {key} here.\")\n",
    "            if key.endswith('_mean'):\n",
    "                # We compute the mean\n",
    "                temp_array = np.array(self.single_cap_features[key])\n",
    "                # Deal with dtype=str case\n",
    "                if np.issubdtype(temp_array.dtype, np.unicode_):\n",
    "                    # Remove empty substring\n",
    "                    temp_array = temp_array[temp_array != '']\n",
    "                    self.single_cap_features[key] = temp_array.astype(float).mean() if not len(temp_array) == 0 else 0\n",
    "                # dtype is not string\n",
    "                else:\n",
    "                    self.single_cap_features[key] = np.array(self.single_cap_features[key]).mean()\n",
    "            elif key.endswith('_std'):\n",
    "                # We compute the Standart Deviation\n",
    "                temp_array = np.array(self.single_cap_features[key])\n",
    "                # Deal with dtype=str case\n",
    "                if np.issubdtype(temp_array.dtype, np.unicode_):\n",
    "                    # Remove empty substring\n",
    "                    temp_array = temp_array[temp_array != '']\n",
    "                    self.single_cap_features[key] = temp_array.astype(float).std() if not len(temp_array) == 0 else 0\n",
    "                # dtype is not string\n",
    "                else:\n",
    "                    self.single_cap_features[key] = np.array(self.single_cap_features[key]).std()\n",
    "            elif key.endswith('_value'):\n",
    "                pass\n",
    "            else:\n",
    "                # We the count for each feature\n",
    "                counter = Counter(self.single_cap_features[key])\n",
    "                self.single_cap_features[key] = counter\n",
    "                if key.endswith('_entropy'):\n",
    "                    # We compute the Shannon entropy\n",
    "                    prob = np.array(list(self.single_cap_features[key].values())) / sum(self.single_cap_features[key].values())\n",
    "                    self.single_cap_features[key] = entropy(prob)\n",
    "\n",
    "    def extract_features(self, final_features_filename: str, train: bool = True, load: bool = False):\n",
    "        \"\"\"\n",
    "        Create a feature dictionary count for each file/capture and then saves it\n",
    "        If load=True, don't compute anything. Just loads the dict from the given filename\n",
    "        \"\"\"\n",
    "        if train:\n",
    "            if load:\n",
    "                self.final_features_train = np.load(final_features_filename)\n",
    "            else:\n",
    "                intermediate_features = []\n",
    "                self.init_cap_features(single=False, train=train)\n",
    "                for i, filename in enumerate(self.filenames_train):\n",
    "                    self.read_single_pcap(filename, train=train)\n",
    "                    intermediate_features.append(copy.deepcopy(self.single_cap_features))\n",
    "                    for key in self.all_cap_features_train.keys():\n",
    "                        if not key.endswith(('_mean', '_std', '_entropy', '_value')):\n",
    "                            self.all_cap_features_train[key] += self.single_cap_features[key]\n",
    "                    \n",
    "                # We now extracted all the features of all the captures\n",
    "                # The mean/std/entropy/value (one per capture) are ready\n",
    "                # For the other features, we now have one dict for each feature for each feature, while we would like a vector -> vectorize\n",
    "                # We have all the possible keys for each feature stored in 'self.all_cap_features_train'\n",
    "                final_features_train = [np.empty(0) for _ in range(self.nb_file_train)]\n",
    "                print('In the final post-processing loop of self.extract_features')\n",
    "                for key in self.all_cap_features_train.keys():\n",
    "                    if key.endswith(('_mean', '_std', '_entropy')): # these keys have already been post-processed\n",
    "                        for i, single_file_features in enumerate(intermediate_features):\n",
    "                            value = single_file_features[key]\n",
    "                            final_features_train[i] = np.concatenate((final_features_train[i], np.array([value])))\n",
    "                    else:\n",
    "                        le_key = LabelEncoder()\n",
    "                        le_key.fit(list(self.all_cap_features_train[key].keys())) # self.all_cap_features_train['src_ip']: Counter({'134.169.109.51': 2188, '134.169.109.25': 1644})\n",
    "                        for i, single_file_features in enumerate(intermediate_features):\n",
    "                            single_file_feature = single_file_features[key]\n",
    "                            final_single_capture = np.zeros(len(le_key.classes_))\n",
    "                            for sub_key in single_file_feature.keys():\n",
    "                                encoded_class = le_key.transform([sub_key])\n",
    "                                final_single_capture[encoded_class] = single_file_feature[sub_key]\n",
    "                            final_features_train[i] = np.concatenate((final_features_train[i], copy.deepcopy(final_single_capture)))\n",
    "    \n",
    "                self.final_features_train = np.array(final_features_train)\n",
    "                np.save(final_features_filename, self.final_features_train)\n",
    "                print(f'{self.final_features_train.shape = }')\n",
    "\n",
    "        # Test mode\n",
    "        else:\n",
    "            if load:\n",
    "                self.final_features_test = np.load(final_features_filename)\n",
    "            else:\n",
    "                intermediate_features = []\n",
    "                self.init_cap_features(single=False, train=train)\n",
    "                for i, filename in enumerate(self.filenames_test):\n",
    "                    print(f'{i = }, {filename = }')\n",
    "                    self.read_single_pcap(filename, train=train)\n",
    "                    intermediate_features.append(copy.deepcopy(self.single_cap_features))\n",
    "                    # We don't need it for test, since our keys are already in self.all_cap_features_train\n",
    "                    # for key in self.all_cap_features_test.keys():\n",
    "                    #     if not key.endswith(('_mean', '_std', '_entropy', '_value')):\n",
    "                    #         self.all_cap_features_test[key] += self.single_cap_features[key]\n",
    "                    \n",
    "                # We now extracted all the features of all the captures\n",
    "                # The mean/std/entropy/value (one per capture) are ready\n",
    "                # For the other features, we now have one dict for each feature for each feature, while we would like a vector -> vectorize\n",
    "                # We have all the possible keys for each feature stored in 'self.all_cap_features_train' (not all_cap_features_test!)\n",
    "                final_features_test = [np.empty(0) for _ in range(self.nb_file_test)]\n",
    "                print('In the final post-processing loop of self.extract_features')\n",
    "                for key in self.all_cap_features_train.keys():\n",
    "                    if key.endswith(('_mean', '_std', '_entropy', '_value')): # these keys have already been post-processed\n",
    "                        for i, single_file_features in enumerate(intermediate_features):\n",
    "                            value = single_file_features[key]\n",
    "                            final_features_test[i] = np.concatenate((final_features_test[i], np.array([value])))\n",
    "                    else:\n",
    "                        le_key = LabelEncoder()\n",
    "                        le_key.fit(list(self.all_cap_features_train[key].keys())) # self.all_cap_features_train['src_ip']: Counter({'134.169.109.51': 2188, '134.169.109.25': 1644})\n",
    "                        for i, single_file_features in enumerate(intermediate_features):\n",
    "                            single_file_feature = single_file_features[key]\n",
    "                            final_single_capture = np.zeros(len(le_key.classes_))\n",
    "                            for sub_key in single_file_feature.keys():\n",
    "                                if sub_key in le_key.classes_:\n",
    "                                    encoded_class = le_key.transform([sub_key])\n",
    "                                    final_single_capture[encoded_class] = single_file_feature[sub_key]\n",
    "                            final_features_test[i] = np.concatenate((final_features_test[i], copy.deepcopy(final_single_capture)))\n",
    "    \n",
    "                self.final_features_test = np.array(final_features_test)\n",
    "                np.save(final_features_filename, self.final_features_test)\n",
    "                print(f'{self.final_features_test.shape = }')\n",
    "\n",
    "torfiles = TorFiles('open-train.tar.gz', 'open-test.tar.gz')\n",
    "torfiles()\n",
    "print(torfiles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "afe479cc-50f6-4dd4-a950-230eda606667",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i = 0, filename = 'train/108.pcap', label = ''\n",
      "i = 1, filename = 'train/109.pcap', label = ''\n",
      "i = 2, filename = 'train/11.pcap', label = ''\n",
      "i = 3, filename = 'train/110.pcap', label = ''\n",
      "One retransmission here.\n",
      "i = 4, filename = 'train/111.pcap', label = ''\n",
      "i = 5, filename = 'train/112.pcap', label = ''\n",
      "i = 6, filename = 'train/113.pcap', label = ''\n",
      "i = 7, filename = 'train/114.pcap', label = ''\n",
      "i = 8, filename = 'train/115.pcap', label = ''\n",
      "i = 9, filename = 'train/116.pcap', label = ''\n",
      "i = 10, filename = 'train/117.pcap', label = ''\n",
      "i = 11, filename = 'train/118.pcap', label = ''\n",
      "i = 12, filename = 'train/119.pcap', label = ''\n",
      "i = 13, filename = 'train/12.pcap', label = ''\n",
      "i = 14, filename = 'train/120.pcap', label = ''\n",
      "i = 15, filename = 'train/121.pcap', label = ''\n",
      "i = 16, filename = 'train/122.pcap', label = ''\n",
      "i = 17, filename = 'train/123.pcap', label = ''\n",
      "i = 18, filename = 'train/124.pcap', label = ''\n",
      "i = 19, filename = 'train/125.pcap', label = ''\n",
      "i = 20, filename = 'train/126.pcap', label = ''\n",
      "i = 21, filename = 'train/127.pcap', label = ''\n",
      "i = 22, filename = 'train/128.pcap', label = ''\n",
      "i = 23, filename = 'train/129.pcap', label = ''\n",
      "i = 24, filename = 'train/13.pcap', label = ''\n",
      "i = 25, filename = 'train/130.pcap', label = ''\n",
      "i = 26, filename = 'train/131.pcap', label = ''\n",
      "i = 27, filename = 'train/132.pcap', label = ''\n",
      "i = 28, filename = 'train/133.pcap', label = ''\n",
      "i = 29, filename = 'train/134.pcap', label = ''\n",
      "i = 30, filename = 'train/135.pcap', label = ''\n",
      "i = 31, filename = 'train/136.pcap', label = ''\n",
      "i = 32, filename = 'train/137.pcap', label = ''\n",
      "i = 33, filename = 'train/138.pcap', label = ''\n",
      "i = 34, filename = 'train/139.pcap', label = ''\n",
      "i = 35, filename = 'train/14.pcap', label = ''\n",
      "i = 36, filename = 'train/140.pcap', label = ''\n",
      "i = 37, filename = 'train/141.pcap', label = ''\n",
      "i = 38, filename = 'train/142.pcap', label = ''\n",
      "i = 39, filename = 'train/143.pcap', label = ''\n",
      "i = 40, filename = 'train/144.pcap', label = ''\n",
      "i = 41, filename = 'train/145.pcap', label = ''\n",
      "i = 42, filename = 'train/146.pcap', label = ''\n",
      "i = 43, filename = 'train/147.pcap', label = ''\n",
      "i = 44, filename = 'train/148.pcap', label = ''\n",
      "i = 45, filename = 'train/149.pcap', label = ''\n",
      "i = 46, filename = 'train/15.pcap', label = ''\n",
      "i = 47, filename = 'train/150.pcap', label = ''\n",
      "i = 48, filename = 'train/151.pcap', label = ''\n",
      "i = 49, filename = 'train/152.pcap', label = ''\n",
      "i = 50, filename = 'train/153.pcap', label = ''\n",
      "i = 51, filename = 'train/154.pcap', label = ''\n",
      "i = 52, filename = 'train/155.pcap', label = ''\n",
      "i = 53, filename = 'train/156.pcap', label = ''\n",
      "i = 54, filename = 'train/157.pcap', label = ''\n",
      "i = 55, filename = 'train/158.pcap', label = ''\n",
      "i = 56, filename = 'train/159.pcap', label = ''\n",
      "i = 57, filename = 'train/16.pcap', label = ''\n",
      "i = 58, filename = 'train/160.pcap', label = ''\n",
      "i = 59, filename = 'train/161.pcap', label = ''\n",
      "i = 68, filename = 'train/17.pcap', label = ''\n",
      "i = 69, filename = 'train/170.pcap', label = ''\n",
      "i = 70, filename = 'train/171.pcap', label = ''\n",
      "i = 71, filename = 'train/172.pcap', label = ''\n",
      "i = 72, filename = 'train/173.pcap', label = ''\n",
      "i = 73, filename = 'train/174.pcap', label = ''\n",
      "i = 74, filename = 'train/175.pcap', label = ''\n",
      "i = 75, filename = 'train/176.pcap', label = ''\n",
      "i = 76, filename = 'train/177.pcap', label = ''\n",
      "One packet_loss here.\n",
      "i = 77, filename = 'train/178.pcap', label = ''\n",
      "i = 78, filename = 'train/179.pcap', label = ''\n",
      "i = 79, filename = 'train/18.pcap', label = ''\n",
      "i = 80, filename = 'train/180.pcap', label = ''\n",
      "i = 81, filename = 'train/181.pcap', label = ''\n",
      "i = 82, filename = 'train/182.pcap', label = ''\n",
      "i = 83, filename = 'train/183.pcap', label = ''\n",
      "i = 84, filename = 'train/184.pcap', label = ''\n",
      "i = 85, filename = 'train/185.pcap', label = ''\n",
      "i = 86, filename = 'train/186.pcap', label = ''\n",
      "i = 87, filename = 'train/187.pcap', label = ''\n",
      "i = 88, filename = 'train/188.pcap', label = ''\n",
      "i = 89, filename = 'train/189.pcap', label = ''\n",
      "i = 90, filename = 'train/19.pcap', label = ''\n",
      "i = 91, filename = 'train/190.pcap', label = ''\n",
      "i = 92, filename = 'train/191.pcap', label = ''\n",
      "i = 93, filename = 'train/192.pcap', label = ''\n",
      "i = 94, filename = 'train/193.pcap', label = ''\n",
      "i = 95, filename = 'train/194.pcap', label = ''\n",
      "i = 96, filename = 'train/195.pcap', label = ''\n",
      "i = 97, filename = 'train/196.pcap', label = ''\n",
      "i = 98, filename = 'train/197.pcap', label = ''\n",
      "i = 99, filename = 'train/198.pcap', label = ''\n",
      "i = 100, filename = 'train/199.pcap', label = ''\n",
      "i = 101, filename = 'train/2.pcap', label = ''\n",
      "i = 102, filename = 'train/20.pcap', label = ''\n",
      "i = 103, filename = 'train/200.pcap', label = ''\n",
      "i = 104, filename = 'train/201.pcap', label = ''\n",
      "i = 105, filename = 'train/202.pcap', label = ''\n",
      "i = 106, filename = 'train/203.pcap', label = ''\n",
      "i = 107, filename = 'train/204.pcap', label = ''\n",
      "i = 108, filename = 'train/205.pcap', label = ''\n",
      "i = 109, filename = 'train/206.pcap', label = ''\n",
      "i = 110, filename = 'train/207.pcap', label = ''\n",
      "i = 111, filename = 'train/208.pcap', label = ''\n",
      "i = 112, filename = 'train/209.pcap', label = ''\n",
      "i = 113, filename = 'train/21.pcap', label = ''\n",
      "One packet_loss here.\n",
      "i = 114, filename = 'train/210.pcap', label = ''\n",
      "i = 115, filename = 'train/211.pcap', label = ''\n",
      "i = 116, filename = 'train/212.pcap', label = ''\n",
      "i = 117, filename = 'train/213.pcap', label = ''\n",
      "i = 118, filename = 'train/214.pcap', label = ''\n",
      "i = 119, filename = 'train/215.pcap', label = ''\n",
      "i = 120, filename = 'train/216.pcap', label = ''\n",
      "i = 121, filename = 'train/217.pcap', label = ''\n",
      "i = 122, filename = 'train/218.pcap', label = ''\n",
      "i = 123, filename = 'train/219.pcap', label = ''\n",
      "i = 124, filename = 'train/22.pcap', label = ''\n",
      "i = 125, filename = 'train/220.pcap', label = ''\n",
      "One retransmission here.\n",
      "i = 126, filename = 'train/221.pcap', label = ''\n",
      "One packet_loss here.\n",
      "i = 127, filename = 'train/222.pcap', label = ''\n",
      "i = 128, filename = 'train/223.pcap', label = ''\n",
      "One packet_loss here.\n",
      "i = 129, filename = 'train/224.pcap', label = ''\n",
      "i = 130, filename = 'train/225.pcap', label = ''\n",
      "i = 131, filename = 'train/226.pcap', label = ''\n",
      "i = 132, filename = 'train/227.pcap', label = ''\n",
      "i = 133, filename = 'train/228.pcap', label = ''\n",
      "i = 134, filename = 'train/229.pcap', label = ''\n",
      "i = 135, filename = 'train/23.pcap', label = ''\n",
      "i = 136, filename = 'train/230.pcap', label = ''\n",
      "i = 137, filename = 'train/231.pcap', label = ''\n",
      "i = 138, filename = 'train/232.pcap', label = ''\n",
      "i = 139, filename = 'train/233.pcap', label = ''\n",
      "i = 140, filename = 'train/234.pcap', label = ''\n",
      "i = 141, filename = 'train/235.pcap', label = ''\n",
      "i = 142, filename = 'train/236.pcap', label = ''\n",
      "i = 143, filename = 'train/237.pcap', label = ''\n",
      "i = 144, filename = 'train/238.pcap', label = ''\n",
      "i = 145, filename = 'train/239.pcap', label = ''\n",
      "i = 146, filename = 'train/24.pcap', label = ''\n",
      "i = 147, filename = 'train/240.pcap', label = ''\n",
      "i = 148, filename = 'train/241.pcap', label = ''\n",
      "i = 149, filename = 'train/242.pcap', label = ''\n",
      "i = 150, filename = 'train/243.pcap', label = ''\n",
      "i = 151, filename = 'train/244.pcap', label = ''\n",
      "i = 152, filename = 'train/245.pcap', label = ''\n",
      "i = 153, filename = 'train/246.pcap', label = ''\n",
      "i = 156, filename = 'train/249.pcap', label = ''\n",
      "i = 157, filename = 'train/25.pcap', label = ''\n",
      "i = 158, filename = 'train/250.pcap', label = ''\n",
      "i = 159, filename = 'train/251.pcap', label = ''\n",
      "i = 160, filename = 'train/252.pcap', label = ''\n",
      "i = 161, filename = 'train/253.pcap', label = ''\n",
      "i = 162, filename = 'train/254.pcap', label = ''\n",
      "i = 163, filename = 'train/255.pcap', label = ''\n",
      "i = 164, filename = 'train/256.pcap', label = ''\n",
      "i = 165, filename = 'train/257.pcap', label = ''\n",
      "i = 166, filename = 'train/258.pcap', label = ''\n",
      "i = 167, filename = 'train/259.pcap', label = ''\n",
      "i = 168, filename = 'train/26.pcap', label = ''\n",
      "i = 169, filename = 'train/260.pcap', label = ''\n",
      "i = 170, filename = 'train/261.pcap', label = ''\n",
      "i = 171, filename = 'train/262.pcap', label = ''\n",
      "i = 172, filename = 'train/263.pcap', label = ''\n",
      "i = 173, filename = 'train/264.pcap', label = ''\n",
      "i = 174, filename = 'train/265.pcap', label = ''\n",
      "i = 175, filename = 'train/266.pcap', label = ''\n",
      "i = 176, filename = 'train/267.pcap', label = ''\n",
      "i = 177, filename = 'train/268.pcap', label = ''\n",
      "i = 178, filename = 'train/269.pcap', label = ''\n",
      "i = 179, filename = 'train/27.pcap', label = ''\n",
      "i = 180, filename = 'train/270.pcap', label = ''\n",
      "i = 181, filename = 'train/271.pcap', label = ''\n",
      "i = 182, filename = 'train/272.pcap', label = ''\n",
      "i = 183, filename = 'train/273.pcap', label = ''\n",
      "i = 184, filename = 'train/274.pcap', label = ''\n",
      "i = 185, filename = 'train/275.pcap', label = ''\n",
      "One packet_loss here.\n",
      "i = 186, filename = 'train/276.pcap', label = ''\n",
      "i = 187, filename = 'train/277.pcap', label = ''\n",
      "i = 188, filename = 'train/278.pcap', label = ''\n",
      "i = 189, filename = 'train/279.pcap', label = ''\n",
      "i = 190, filename = 'train/28.pcap', label = ''\n",
      "i = 191, filename = 'train/280.pcap', label = ''\n",
      "i = 192, filename = 'train/281.pcap', label = ''\n",
      "i = 193, filename = 'train/282.pcap', label = ''\n",
      "i = 194, filename = 'train/283.pcap', label = ''\n",
      "i = 195, filename = 'train/284.pcap', label = ''\n",
      "i = 196, filename = 'train/285.pcap', label = ''\n",
      "i = 197, filename = 'train/286.pcap', label = ''\n",
      "i = 198, filename = 'train/287.pcap', label = ''\n",
      "i = 199, filename = 'train/288.pcap', label = ''\n",
      "i = 200, filename = 'train/289.pcap', label = ''\n",
      "i = 201, filename = 'train/29.pcap', label = ''\n",
      "i = 202, filename = 'train/290.pcap', label = ''\n",
      "i = 203, filename = 'train/291.pcap', label = ''\n",
      "i = 204, filename = 'train/292.pcap', label = ''\n",
      "i = 205, filename = 'train/293.pcap', label = ''\n",
      "i = 206, filename = 'train/294.pcap', label = ''\n",
      "i = 207, filename = 'train/295.pcap', label = ''\n",
      "i = 208, filename = 'train/296.pcap', label = ''\n",
      "i = 209, filename = 'train/297.pcap', label = ''\n",
      "i = 210, filename = 'train/298.pcap', label = ''\n",
      "i = 211, filename = 'train/299.pcap', label = ''\n",
      "i = 212, filename = 'train/3.pcap', label = ''\n",
      "i = 213, filename = 'train/30.pcap', label = ''\n",
      "i = 214, filename = 'train/300.pcap', label = ''\n",
      "i = 215, filename = 'train/301.pcap', label = ''\n",
      "i = 216, filename = 'train/302.pcap', label = ''\n",
      "i = 217, filename = 'train/303.pcap', label = ''\n",
      "i = 218, filename = 'train/304.pcap', label = ''\n",
      "i = 219, filename = 'train/305.pcap', label = ''\n",
      "i = 220, filename = 'train/306.pcap', label = ''\n",
      "i = 221, filename = 'train/307.pcap', label = ''\n",
      "i = 222, filename = 'train/308.pcap', label = ''\n",
      "i = 223, filename = 'train/309.pcap', label = ''\n",
      "One packet_loss here.\n",
      "i = 224, filename = 'train/31.pcap', label = ''\n",
      "i = 234, filename = 'train/319.pcap', label = ''\n",
      "i = 235, filename = 'train/32.pcap', label = ''\n",
      "i = 236, filename = 'train/320.pcap', label = ''\n",
      "i = 237, filename = 'train/321.pcap', label = ''\n",
      "i = 238, filename = 'train/322.pcap', label = ''\n",
      "i = 239, filename = 'train/323.pcap', label = ''\n",
      "i = 240, filename = 'train/324.pcap', label = ''\n",
      "i = 241, filename = 'train/325.pcap', label = ''\n",
      "i = 242, filename = 'train/326.pcap', label = ''\n",
      "i = 243, filename = 'train/327.pcap', label = ''\n",
      "i = 244, filename = 'train/328.pcap', label = ''\n",
      "i = 245, filename = 'train/329.pcap', label = ''\n",
      "i = 246, filename = 'train/33.pcap', label = ''\n",
      "i = 247, filename = 'train/330.pcap', label = ''\n",
      "i = 248, filename = 'train/331.pcap', label = ''\n",
      "i = 249, filename = 'train/332.pcap', label = ''\n",
      "i = 250, filename = 'train/333.pcap', label = ''\n",
      "i = 251, filename = 'train/334.pcap', label = ''\n",
      "i = 252, filename = 'train/335.pcap', label = ''\n",
      "i = 253, filename = 'train/336.pcap', label = ''\n",
      "i = 254, filename = 'train/337.pcap', label = ''\n",
      "i = 255, filename = 'train/338.pcap', label = ''\n",
      "i = 256, filename = 'train/339.pcap', label = ''\n",
      "i = 257, filename = 'train/34.pcap', label = ''\n",
      "i = 258, filename = 'train/340.pcap', label = ''\n",
      "i = 259, filename = 'train/341.pcap', label = ''\n",
      "i = 260, filename = 'train/342.pcap', label = ''\n",
      "i = 261, filename = 'train/343.pcap', label = ''\n",
      "i = 262, filename = 'train/344.pcap', label = ''\n",
      "i = 263, filename = 'train/345.pcap', label = ''\n",
      "i = 264, filename = 'train/346.pcap', label = ''\n",
      "i = 265, filename = 'train/347.pcap', label = ''\n",
      "i = 266, filename = 'train/348.pcap', label = ''\n",
      "i = 267, filename = 'train/349.pcap', label = ''\n",
      "i = 268, filename = 'train/35.pcap', label = ''\n",
      "i = 269, filename = 'train/350.pcap', label = ''\n",
      "i = 280, filename = 'train/360.pcap', label = ''\n",
      "i = 281, filename = 'train/361.pcap', label = ''\n",
      "i = 282, filename = 'train/362.pcap', label = ''\n",
      "i = 283, filename = 'train/363.pcap', label = ''\n",
      "i = 284, filename = 'train/364.pcap', label = ''\n",
      "i = 285, filename = 'train/365.pcap', label = ''\n",
      "i = 286, filename = 'train/366.pcap', label = ''\n",
      "i = 287, filename = 'train/367.pcap', label = ''\n",
      "i = 288, filename = 'train/368.pcap', label = ''\n",
      "i = 289, filename = 'train/369.pcap', label = ''\n",
      "i = 290, filename = 'train/37.pcap', label = ''\n",
      "i = 291, filename = 'train/370.pcap', label = ''\n",
      "i = 292, filename = 'train/371.pcap', label = ''\n",
      "i = 293, filename = 'train/372.pcap', label = ''\n",
      "i = 294, filename = 'train/373.pcap', label = ''\n",
      "i = 295, filename = 'train/374.pcap', label = ''\n",
      "i = 296, filename = 'train/375.pcap', label = ''\n",
      "i = 297, filename = 'train/376.pcap', label = ''\n",
      "i = 298, filename = 'train/377.pcap', label = ''\n",
      "i = 299, filename = 'train/378.pcap', label = ''\n",
      "i = 300, filename = 'train/379.pcap', label = ''\n",
      "i = 301, filename = 'train/38.pcap', label = ''\n",
      "i = 302, filename = 'train/380.pcap', label = ''\n",
      "i = 303, filename = 'train/381.pcap', label = ''\n",
      "i = 304, filename = 'train/382.pcap', label = ''\n",
      "i = 305, filename = 'train/383.pcap', label = ''\n",
      "i = 306, filename = 'train/384.pcap', label = ''\n",
      "i = 307, filename = 'train/385.pcap', label = ''\n",
      "i = 308, filename = 'train/386.pcap', label = ''\n",
      "i = 309, filename = 'train/387.pcap', label = ''\n",
      "i = 310, filename = 'train/388.pcap', label = ''\n",
      "i = 311, filename = 'train/389.pcap', label = ''\n",
      "i = 312, filename = 'train/39.pcap', label = ''\n",
      "i = 313, filename = 'train/390.pcap', label = ''\n",
      "i = 314, filename = 'train/391.pcap', label = ''\n",
      "i = 315, filename = 'train/392.pcap', label = ''\n",
      "i = 316, filename = 'train/393.pcap', label = ''\n",
      "i = 317, filename = 'train/394.pcap', label = ''\n",
      "i = 318, filename = 'train/395.pcap', label = ''\n",
      "i = 319, filename = 'train/396.pcap', label = ''\n",
      "i = 320, filename = 'train/397.pcap', label = ''\n",
      "i = 321, filename = 'train/398.pcap', label = ''\n",
      "i = 322, filename = 'train/399.pcap', label = ''\n",
      "i = 323, filename = 'train/4.pcap', label = ''\n",
      "i = 324, filename = 'train/40.pcap', label = ''\n",
      "i = 325, filename = 'train/400.pcap', label = ''\n",
      "i = 326, filename = 'train/401.pcap', label = ''\n",
      "i = 327, filename = 'train/402.pcap', label = ''\n",
      "i = 328, filename = 'train/403.pcap', label = ''\n",
      "i = 329, filename = 'train/404.pcap', label = ''\n",
      "i = 330, filename = 'train/405.pcap', label = ''\n",
      "i = 331, filename = 'train/406.pcap', label = ''\n",
      "One retransmission here.\n",
      "i = 332, filename = 'train/407.pcap', label = ''\n",
      "i = 333, filename = 'train/408.pcap', label = ''\n",
      "i = 334, filename = 'train/409.pcap', label = ''\n",
      "i = 335, filename = 'train/41.pcap', label = ''\n",
      "i = 336, filename = 'train/410.pcap', label = ''\n",
      "i = 337, filename = 'train/411.pcap', label = ''\n",
      "i = 352, filename = 'train/425.pcap', label = ''\n",
      "i = 353, filename = 'train/426.pcap', label = ''\n",
      "i = 354, filename = 'train/427.pcap', label = ''\n",
      "i = 355, filename = 'train/428.pcap', label = ''\n",
      "i = 356, filename = 'train/429.pcap', label = ''\n",
      "i = 357, filename = 'train/43.pcap', label = ''\n",
      "i = 358, filename = 'train/430.pcap', label = ''\n",
      "i = 359, filename = 'train/431.pcap', label = ''\n",
      "i = 360, filename = 'train/432.pcap', label = ''\n",
      "i = 361, filename = 'train/433.pcap', label = ''\n",
      "i = 362, filename = 'train/434.pcap', label = ''\n",
      "i = 363, filename = 'train/435.pcap', label = ''\n",
      "i = 364, filename = 'train/436.pcap', label = ''\n",
      "i = 365, filename = 'train/437.pcap', label = ''\n",
      "i = 366, filename = 'train/438.pcap', label = ''\n",
      "i = 367, filename = 'train/439.pcap', label = ''\n",
      "i = 368, filename = 'train/44.pcap', label = ''\n",
      "i = 369, filename = 'train/440.pcap', label = ''\n",
      "i = 370, filename = 'train/441.pcap', label = ''\n",
      "i = 371, filename = 'train/442.pcap', label = ''\n",
      "i = 372, filename = 'train/443.pcap', label = ''\n",
      "i = 373, filename = 'train/444.pcap', label = ''\n",
      "i = 374, filename = 'train/445.pcap', label = ''\n",
      "i = 375, filename = 'train/446.pcap', label = ''\n",
      "i = 376, filename = 'train/447.pcap', label = ''\n",
      "i = 377, filename = 'train/448.pcap', label = ''\n",
      "i = 378, filename = 'train/449.pcap', label = ''\n",
      "i = 379, filename = 'train/45.pcap', label = ''\n",
      "i = 380, filename = 'train/450.pcap', label = ''\n",
      "i = 381, filename = 'train/451.pcap', label = ''\n",
      "i = 382, filename = 'train/452.pcap', label = ''\n",
      "i = 383, filename = 'train/453.pcap', label = ''\n",
      "i = 384, filename = 'train/454.pcap', label = ''\n",
      "i = 385, filename = 'train/455.pcap', label = ''\n",
      "i = 386, filename = 'train/456.pcap', label = ''\n",
      "i = 387, filename = 'train/457.pcap', label = ''\n",
      "i = 388, filename = 'train/458.pcap', label = ''\n",
      "i = 389, filename = 'train/459.pcap', label = ''\n",
      "i = 390, filename = 'train/46.pcap', label = ''\n",
      "i = 391, filename = 'train/460.pcap', label = ''\n",
      "i = 392, filename = 'train/461.pcap', label = ''\n",
      "i = 393, filename = 'train/462.pcap', label = ''\n",
      "i = 394, filename = 'train/463.pcap', label = ''\n",
      "i = 395, filename = 'train/464.pcap', label = ''\n",
      "i = 396, filename = 'train/465.pcap', label = ''\n",
      "i = 397, filename = 'train/466.pcap', label = ''\n",
      "i = 398, filename = 'train/467.pcap', label = ''\n",
      "i = 399, filename = 'train/468.pcap', label = ''\n",
      "i = 400, filename = 'train/469.pcap', label = ''\n",
      "i = 406, filename = 'train/474.pcap', label = ''\n",
      "i = 407, filename = 'train/475.pcap', label = ''\n",
      "One retransmission here.\n",
      "i = 408, filename = 'train/476.pcap', label = ''\n",
      "i = 409, filename = 'train/477.pcap', label = ''\n",
      "i = 410, filename = 'train/478.pcap', label = ''\n",
      "i = 411, filename = 'train/479.pcap', label = ''\n",
      "i = 412, filename = 'train/48.pcap', label = ''\n",
      "i = 413, filename = 'train/480.pcap', label = ''\n",
      "i = 414, filename = 'train/481.pcap', label = ''\n",
      "i = 415, filename = 'train/482.pcap', label = ''\n",
      "i = 416, filename = 'train/483.pcap', label = ''\n",
      "i = 417, filename = 'train/484.pcap', label = ''\n",
      "i = 418, filename = 'train/485.pcap', label = ''\n",
      "i = 419, filename = 'train/486.pcap', label = ''\n",
      "i = 420, filename = 'train/487.pcap', label = ''\n",
      "i = 421, filename = 'train/488.pcap', label = ''\n",
      "i = 422, filename = 'train/489.pcap', label = ''\n",
      "i = 423, filename = 'train/49.pcap', label = ''\n",
      "i = 424, filename = 'train/490.pcap', label = ''\n",
      "i = 425, filename = 'train/491.pcap', label = ''\n",
      "i = 426, filename = 'train/492.pcap', label = ''\n",
      "i = 427, filename = 'train/493.pcap', label = ''\n",
      "i = 428, filename = 'train/494.pcap', label = ''\n",
      "i = 429, filename = 'train/495.pcap', label = ''\n",
      "i = 430, filename = 'train/496.pcap', label = ''\n",
      "i = 431, filename = 'train/497.pcap', label = ''\n",
      "i = 432, filename = 'train/498.pcap', label = ''\n",
      "i = 433, filename = 'train/499.pcap', label = ''\n",
      "i = 434, filename = 'train/5.pcap', label = ''\n",
      "i = 435, filename = 'train/50.pcap', label = ''\n",
      "i = 436, filename = 'train/500.pcap', label = ''\n",
      "i = 437, filename = 'train/501.pcap', label = ''\n",
      "i = 438, filename = 'train/502.pcap', label = ''\n",
      "i = 439, filename = 'train/503.pcap', label = ''\n",
      "i = 440, filename = 'train/504.pcap', label = ''\n",
      "i = 441, filename = 'train/505.pcap', label = ''\n",
      "i = 442, filename = 'train/506.pcap', label = ''\n",
      "i = 443, filename = 'train/507.pcap', label = ''\n",
      "i = 444, filename = 'train/508.pcap', label = ''\n",
      "i = 445, filename = 'train/509.pcap', label = ''\n",
      "i = 446, filename = 'train/51.pcap', label = ''\n",
      "i = 447, filename = 'train/510.pcap', label = ''\n",
      "i = 448, filename = 'train/511.pcap', label = ''\n",
      "i = 449, filename = 'train/512.pcap', label = ''\n",
      "i = 450, filename = 'train/513.pcap', label = ''\n",
      "i = 451, filename = 'train/514.pcap', label = ''\n",
      "i = 452, filename = 'train/515.pcap', label = ''\n",
      "i = 453, filename = 'train/516.pcap', label = ''\n",
      "i = 454, filename = 'train/517.pcap', label = ''\n",
      "i = 455, filename = 'train/518.pcap', label = ''\n",
      "i = 456, filename = 'train/519.pcap', label = ''\n",
      "i = 457, filename = 'train/52.pcap', label = ''\n",
      "i = 458, filename = 'train/520.pcap', label = ''\n",
      "i = 459, filename = 'train/521.pcap', label = ''\n",
      "i = 460, filename = 'train/522.pcap', label = ''\n",
      "i = 461, filename = 'train/523.pcap', label = ''\n",
      "i = 462, filename = 'train/524.pcap', label = ''\n",
      "i = 463, filename = 'train/525.pcap', label = ''\n",
      "i = 464, filename = 'train/526.pcap', label = ''\n",
      "i = 465, filename = 'train/527.pcap', label = ''\n",
      "i = 466, filename = 'train/528.pcap', label = ''\n",
      "i = 467, filename = 'train/529.pcap', label = ''\n",
      "i = 468, filename = 'train/53.pcap', label = ''\n",
      "i = 469, filename = 'train/530.pcap', label = ''\n",
      "i = 470, filename = 'train/531.pcap', label = ''\n",
      "i = 471, filename = 'train/532.pcap', label = ''\n",
      "i = 472, filename = 'train/533.pcap', label = ''\n",
      "i = 473, filename = 'train/534.pcap', label = ''\n",
      "i = 474, filename = 'train/535.pcap', label = ''\n",
      "i = 475, filename = 'train/536.pcap', label = ''\n",
      "i = 476, filename = 'train/537.pcap', label = ''\n",
      "i = 477, filename = 'train/538.pcap', label = ''\n",
      "i = 478, filename = 'train/539.pcap', label = ''\n",
      "i = 479, filename = 'train/54.pcap', label = ''\n",
      "i = 480, filename = 'train/540.pcap', label = ''\n",
      "i = 481, filename = 'train/541.pcap', label = ''\n",
      "i = 482, filename = 'train/542.pcap', label = ''\n",
      "i = 483, filename = 'train/543.pcap', label = ''\n",
      "i = 484, filename = 'train/544.pcap', label = ''\n",
      "i = 485, filename = 'train/545.pcap', label = ''\n",
      "i = 486, filename = 'train/546.pcap', label = ''\n",
      "i = 487, filename = 'train/547.pcap', label = ''\n",
      "i = 488, filename = 'train/548.pcap', label = ''\n",
      "i = 489, filename = 'train/549.pcap', label = ''\n",
      "i = 490, filename = 'train/55.pcap', label = ''\n",
      "i = 491, filename = 'train/550.pcap', label = ''\n",
      "i = 492, filename = 'train/551.pcap', label = ''\n",
      "i = 493, filename = 'train/552.pcap', label = ''\n",
      "i = 494, filename = 'train/553.pcap', label = ''\n",
      "i = 495, filename = 'train/554.pcap', label = ''\n",
      "i = 496, filename = 'train/555.pcap', label = ''\n",
      "i = 497, filename = 'train/556.pcap', label = ''\n",
      "i = 498, filename = 'train/557.pcap', label = ''\n",
      "i = 499, filename = 'train/558.pcap', label = ''\n",
      "i = 500, filename = 'train/559.pcap', label = ''\n",
      "i = 501, filename = 'train/56.pcap', label = ''\n",
      "One packet_loss here.\n",
      "i = 502, filename = 'train/560.pcap', label = ''\n",
      "i = 506, filename = 'train/564.pcap', label = ''\n",
      "i = 507, filename = 'train/565.pcap', label = ''\n",
      "i = 508, filename = 'train/566.pcap', label = ''\n",
      "i = 509, filename = 'train/567.pcap', label = ''\n",
      "i = 510, filename = 'train/568.pcap', label = ''\n",
      "i = 511, filename = 'train/569.pcap', label = ''\n",
      "i = 512, filename = 'train/57.pcap', label = ''\n",
      "i = 513, filename = 'train/570.pcap', label = ''\n",
      "i = 514, filename = 'train/571.pcap', label = ''\n",
      "i = 515, filename = 'train/572.pcap', label = ''\n",
      "i = 516, filename = 'train/573.pcap', label = ''\n",
      "i = 517, filename = 'train/574.pcap', label = ''\n",
      "i = 518, filename = 'train/575.pcap', label = ''\n",
      "i = 519, filename = 'train/576.pcap', label = ''\n",
      "i = 520, filename = 'train/577.pcap', label = ''\n",
      "i = 521, filename = 'train/578.pcap', label = ''\n",
      "i = 522, filename = 'train/579.pcap', label = ''\n",
      "i = 523, filename = 'train/58.pcap', label = ''\n",
      "i = 524, filename = 'train/580.pcap', label = ''\n",
      "i = 525, filename = 'train/581.pcap', label = ''\n",
      "i = 526, filename = 'train/582.pcap', label = ''\n",
      "i = 527, filename = 'train/583.pcap', label = ''\n",
      "i = 528, filename = 'train/584.pcap', label = ''\n",
      "i = 529, filename = 'train/585.pcap', label = ''\n",
      "i = 530, filename = 'train/586.pcap', label = ''\n",
      "i = 531, filename = 'train/587.pcap', label = ''\n",
      "i = 532, filename = 'train/588.pcap', label = ''\n",
      "i = 533, filename = 'train/589.pcap', label = ''\n",
      "i = 534, filename = 'train/59.pcap', label = ''\n",
      "i = 535, filename = 'train/590.pcap', label = ''\n",
      "i = 536, filename = 'train/591.pcap', label = ''\n",
      "One packet_loss here.\n",
      "i = 537, filename = 'train/592.pcap', label = ''\n",
      "i = 538, filename = 'train/593.pcap', label = ''\n",
      "i = 539, filename = 'train/594.pcap', label = ''\n",
      "i = 540, filename = 'train/595.pcap', label = ''\n",
      "i = 541, filename = 'train/596.pcap', label = ''\n",
      "i = 542, filename = 'train/597.pcap', label = ''\n",
      "i = 543, filename = 'train/598.pcap', label = ''\n",
      "i = 544, filename = 'train/599.pcap', label = ''\n",
      "i = 545, filename = 'train/6.pcap', label = ''\n",
      "One retransmission here.\n",
      "i = 546, filename = 'train/60.pcap', label = ''\n",
      "i = 547, filename = 'train/600.pcap', label = ''\n",
      "i = 548, filename = 'train/61.pcap', label = ''\n",
      "i = 549, filename = 'train/62.pcap', label = ''\n",
      "i = 550, filename = 'train/63.pcap', label = ''\n",
      "i = 551, filename = 'train/64.pcap', label = ''\n",
      "i = 552, filename = 'train/65.pcap', label = ''\n",
      "i = 553, filename = 'train/66.pcap', label = ''\n",
      "i = 554, filename = 'train/67.pcap', label = ''\n",
      "i = 555, filename = 'train/68.pcap', label = ''\n",
      "i = 556, filename = 'train/69.pcap', label = ''\n",
      "i = 557, filename = 'train/7.pcap', label = ''\n",
      "i = 581, filename = 'train/91.pcap', label = ''\n",
      "i = 582, filename = 'train/92.pcap', label = ''\n",
      "i = 583, filename = 'train/93.pcap', label = ''\n",
      "i = 584, filename = 'train/94.pcap', label = ''\n",
      "i = 585, filename = 'train/95.pcap', label = ''\n",
      "i = 586, filename = 'train/96.pcap', label = ''\n",
      "i = 587, filename = 'train/97.pcap', label = ''\n",
      "i = 588, filename = 'train/98.pcap', label = ''\n",
      "i = 589, filename = 'train/99.pcap', label = ''\n",
      "In the final post-processing loop of self.extract_features\n",
      "self.final_features_train.shape = (590, 2357)\n"
     ]
    }
   ],
   "source": [
    "# torfiles.extract_features('final_features_train.npy', train=True, load=False)\n",
    "torfiles.extract_features('final_features_train_10.npy', train=True, load=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "361eea11-1df1-4865-af17-55f473c9b6c0",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## 2. Model training and selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "3af3e91e-381a-4d2a-8e3f-c51f05e1c2bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "###\n",
    "# One-Class SVM\n",
    "###\n",
    "\n",
    "# ocsvm_model = OneClassSVM()\n",
    "ocsvm_model = OneClassSVM(nu=0.5, kernel=\"rbf\", gamma='scale')\n",
    "ocsvm_model.fit(torfiles.final_features_train)\n",
    "y_pred0_oc = ocsvm_model.predict(torfiles.final_features_train)\n",
    "# Get the features of task 1, that is the censored websites, to see if they all are detcted as outliers\n",
    "# y_pred1 = ocsvm_model.predict(np.concatenate((np.load('final_features_train1.npy'), np.load('final_features_test1.npy'))))\n",
    "# print(f'Accuracy of {\"One-Class SVM\"}: {((y_pred0==0).sum() + (y_pred1==1).sum()) / (len(y_pred0)+len(y_pred1)) * 100:.2f}%')\n",
    "\n",
    "###\n",
    "# Isolation Forest\n",
    "###\n",
    "\n",
    "if_model = IsolationForest(contamination=0.1)\n",
    "if_model.fit(torfiles.final_features_train)\n",
    "y_pred0_if = if_model.predict(torfiles.final_features_train)\n",
    "# Get the features of task 1, that is the censored websites, to see if they all are detcted as outliers\n",
    "# y_pred1 = if_model.predict(np.concatenate((np.load('final_features_train1.npy'), np.load('final_features_test1.npy'))))\n",
    "# print(f'Accuracy of {\"Isolation Forest\"}: {((y_pred0==0).sum() + (y_pred1==1).sum()) / (len(y_pred0)+len(y_pred1)) * 100:.2f}%')\n",
    "\n",
    "###\n",
    "# Local Outlier Factor (LOF)\n",
    "###\n",
    "\n",
    "# lof_model = LocalOutlierFactor(n_neighbors=600, contamination=0.0001)\n",
    "lof_model = LocalOutlierFactor(n_neighbors=20, contamination=0.0001, novelty=True)\n",
    "lof_model.fit(torfiles.final_features_train)\n",
    "y_pred0 = lof_model.predict(torfiles.final_features_train)\n",
    "# Get the features of task 1, that is the censored websites, to see if they all are detcted as outliers\n",
    "# y_pred1 = lof_model.predict(np.concatenate((np.load('final_features_train1.npy'), np.load('final_features_test1.npy'))))\n",
    "# print(f'Accuracy of {\"Local Outlier Factor (LOF)\"}: {((y_pred0==0).sum() + (y_pred1==1).sum()) / (len(y_pred0)+len(y_pred1)) * 100:.2f}%')\n",
    "\n",
    "###\n",
    "# DBSCAN\n",
    "###\n",
    "\n",
    "# dbscan_model = DBSCAN(eps=.1)\n",
    "# dbscan_model.fit(torfiles.final_features_train)\n",
    "# y_pred0 = dbscan_model.predict(torfiles.final_features_train)\n",
    "# Get the features of task 1, that is the censored websites, to see if they all are detcted as outliers\n",
    "# y_pred1 = dbscan_model.predict(np.concatenate((np.load('final_features_train1.npy'), np.load('final_features_test1.npy'))))\n",
    "# print(f'Accuracy of {\"DBSCAN\"}: {((y_pred0==0).sum() + (y_pred1==1).sum()) / (len(y_pred0)+len(y_pred1)) * 100:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48adfacf-9a36-4ca3-b28e-d8b239f2e3b4",
   "metadata": {},
   "source": [
    "## 3. Predition on test dataset + save submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "3d256f17-7b72-4f67-a658-06ecdd2fb1f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "296"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(y_pred0_oc == -1).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "7a4e0939-c046-430b-865e-bbc30a42eaf5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "##### Extraction of test features #####\n",
    "\n",
    "# torfiles.extract_features('final_features_test.npy', train=False, load=False)\n",
    "# torfiles.extract_features('final_features_test_2.npy', train=False, load=False)\n",
    "\n",
    "##### Predict of test dataset  #####\n",
    "\n",
    "X_test = features_test\n",
    "y_pred = ocsvm_model.predict(X_test)\n",
    "\n",
    "# Code specific to unit4-fingerprint.ipynb\n",
    "# with open('class_dict.json', 'r') as file:\n",
    "#     class_encoder_lukas = json.load(file)\n",
    "# y_pred = torfiles.decode_labels(y_pred)\n",
    "# y_pred = [class_encoder_lukas[website] for website in y_pred]\n",
    "# y_pred = np.array(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "81f5fce6-d5d5-4a79-ba70-fe275976f6cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "307"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(y_pred == -1).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "955c88d5-c669-4d99-b3c0-14296f774ca8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "600\n",
      "Length of our submission: 600 | Length of zip file: 600\n"
     ]
    }
   ],
   "source": [
    "##### Save submission #####\n",
    "\n",
    "submission = []\n",
    "# Write the prediction as expected output\n",
    "with tarfile.open('open-test.tar.gz', 'r:gz') as tar:\n",
    "    filenames = tar.getnames()\n",
    "    if filenames[0] == 'test':\n",
    "        filenames = filenames[1:]\n",
    "filenames_test = filenames\n",
    "print(len(filenames_test))\n",
    "\n",
    "for i, filename in enumerate(filenames_test):\n",
    "    submission += [filename[filename.find('/') + 1:] + ';' + ('1' if y_pred[i] == -1 else '0')]\n",
    "# Save the output as a text file\n",
    "print(f'Length of our submission: {len(submission)} | Length of zip file: {len(filenames_test)}')\n",
    "np.savetxt('output_ocsvm_600_05.csv', np.array(submission), fmt='%s', delimiter=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e274a73-e4f4-4c1c-bd3d-140da7d5816a",
   "metadata": {},
   "source": [
    "### Deal with 10 missing files pb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "26b27e0e-23f3-481a-9b8e-2fdf0fa4ff15",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Saved\n",
    "final_features_test = torfiles.final_features_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "e2ae643b-04fe-440a-bb2c-744ebb0d1e01",
   "metadata": {},
   "outputs": [],
   "source": [
    "with tarfile.open('open-test.tar.gz', 'r:gz') as tar:\n",
    "    filenames = tar.getnames()\n",
    "    if filenames[0] == 'test':\n",
    "        filenames = filenames[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "c1d6f133-cefa-41f6-93b3-f4ba2b6db6bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['test/1.pcap',\n",
       " 'test/10.pcap',\n",
       " 'test/100.pcap',\n",
       " 'test/101.pcap',\n",
       " 'test/102.pcap',\n",
       " 'test/103.pcap',\n",
       " 'test/104.pcap',\n",
       " 'test/105.pcap',\n",
       " 'test/106.pcap']"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filenames_rem = filenames[:9]\n",
    "torfiles.filenames_test = filenames_rem\n",
    "torfiles.filenames_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "edef043c-e3e0-4428-9ab9-35d63f160b85",
   "metadata": {},
   "outputs": [],
   "source": [
    "torfiles.nb_file_test = 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "b42f7e2e-4190-4f8e-b4ad-a77155e8fc21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i = 0, filename = 'test/1.pcap'\n",
      "i = 1, filename = 'test/10.pcap'\n",
      "i = 2, filename = 'test/100.pcap'\n",
      "i = 3, filename = 'test/101.pcap'\n",
      "i = 4, filename = 'test/102.pcap'\n",
      "i = 5, filename = 'test/103.pcap'\n",
      "i = 6, filename = 'test/104.pcap'\n",
      "i = 7, filename = 'test/105.pcap'\n",
      "i = 8, filename = 'test/106.pcap'\n",
      "In the final post-processing loop of self.extract_features\n",
      "self.final_features_test.shape = (9, 2357)\n"
     ]
    }
   ],
   "source": [
    "torfiles.extract_features('final_features_test_rem.npy', train=False, load=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "baf952bf-256a-49d9-b08c-8570379ed25b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9, 2357)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_features_test_rem = torfiles.final_features_test\n",
    "final_features_test_rem.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "fae2c313-4d80-4fdd-b522-519fd5bb80f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(600, 2357)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features_test = np.concatenate((final_features_test_rem, final_features_test), axis=0)\n",
    "features_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "04a4b559-6693-4b9e-9e5e-bc2055f8c276",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('final_features_test_9_591.npy', features_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84bb5f5d-0599-40d2-aba9-4e9de7e4c8c3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
